---
title: "PHP2530 Problem Set 4"
author: "Alyson Singleton"
date: "5/7/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, results='asis', warning=F, message=F, cache=T,
                      fig.height=4, fig.width=5, fig.align="center")
pacman::p_load(actuar, dplyr, kableExtra, knitr, LearnBayes, boot, lattice, gtools, ggplot2,
               LaplacesDemon, rstan, rstanarm, bayesplot)
```

## 1. The Importance Sampling Algorithm
#### Part A
I used the following code as guidance but created my own version of the algorithm with explanations of each step to appropriately display my understanding and effort [http://sites.fas.harvard.edu/~stat221/ProblemSets/hw3_sols_code.R]. I also used your suggested reading [https://www.tandfonline.com/doi/pdf/10.1080/01621459.1998.10473764?casa_token=7igBF0N06HQAAAAA:PcNYjS0NX7gRUCRUzXvW7bMHAQ8r9aQsiXHBukKWOUV0KA5W_Fc-gIOd7ZoFHX3gFrxYI8EVHaYrog]. **Could change while loop and output into a dataframe instead of list of vectors.

```{r, imp.sampA, echo=T}
#######################################
# 1. The Importance Sampling Algorithm
ImpSampler <- function(nSamples, logTargetDensityFunc, logProposalDensityFunc, 
                       proposalNewFunc, rejectionControlConstant = NULL) {
  # first check what's up with the rejectionControlConstant, i.e. is it null? if yes then ...
  if (is.null(rejectionControlConstant)) {
    # initialize samples vector w proposalnewfunc values (i.e. N(0,3^2))
    samples.vec <- rep(NA,nSamples)
    for (i in 1:nSamples){
      samples.vec[i] <- proposalNewFunc()
    }
    # initialize and find log weights
    log.weights.vec <- rep(NA,nSamples) 
    final.log.weights.vec <- sapply(samples.vec, logTargetDensityFunc) - 
      sapply(samples.vec, logProposalDensityFunc)
    # store samples for output
    final.samples <- samples.vec
    # acceptance rate doesn't apply here
    acceptance.rate <- NA
    # calculated estimated ESS as directed
    estimated.ESS <- length(final.log.weights.vec) / 
      (1 + var(exp(final.log.weights.vec)))
  # if rejectionControlConstant was not null then  
  }else{ 
    #initialize storage
    list.log.ratios = c()
    final.log.weights.vec = c()
    final.samples = c()
    # keep going until we've found the number of samples we want
    while (length(final.samples) < nSamples) {
      # again initialize samples vector w proposalnewfunc values (i.e. N(0,3^2))
      samples.vec <- rep(NA,nSamples)
      for (i in 1:nSamples){
        samples.vec[i] <- proposalNewFunc()
      }
      # again initialize and find log weights
      log.weights.vec <- rep(NA,nSamples) 
      log.weights.vec <- sapply(samples.vec, logTargetDensityFunc) - 
        sapply(samples.vec, logProposalDensityFunc)
      
      # now perform rejection control across all weights
      # we're working on the log scale so subtract log("c" val)
      log.ratios <- log.weights.vec - log(rejectionControlConstant)
      # take log(min(1,w/"c")) i.e. min(0,log(weight/"c"))
      log.ratios <- ifelse(log.ratios>0,0,log.ratios)
      # accept or reject with calculated probability, again on log scale
      acceptance.bool <- (log(runif(nSamples)) < log.ratios)
      # add those that pass to the list of final samples
      final.samples <- c(final.samples, samples.vec[acceptance.bool])
      # store the log.ratios that we tried
      list.log.ratios <- c(list.log.ratios, log.ratios)
      # update weights (w/r) and add to list of final weights
      final.log.weights.vec <- c(final.log.weights.vec, 
                                 log.weights.vec[acceptance.bool] - 
                                   log.ratios[acceptance.bool])
    }
    #calculate acceptance rate
    acceptance.rate <- length(final.samples)/length(list.log.ratios)
    #make sure we have no extra samples (chop off the extras if we do), 
        #and make final update to weights (p*w/r)
    final.log.weights.vec <- final.log.weights.vec[1:nSamples] + 
      log(mean(exp(list.log.ratios)))
    #again remove extra samples
    final.samples <- final.samples[1:nSamples]
    #calculate the estimated ESS
    estimated.ESS <- nSamples/(1 + var(exp(final.log.weights.vec))) 
  }
  return(list(final.samples, final.log.weights.vec, estimated.ESS, acceptance.rate))
}
```

#### Part B/C
See the requested plot below. Indeed, the suggested g(x) does seem like an appropriate importance density function as it will help us identify the modes of our mixture model.
 
```{r, imp.sampBC}
test.values <- seq(-6,6,.01)
fx.normal.samples <- (1/3)*dnorm(test.values,mean=-2,sd=1) + (2/3)*dnorm(test.values,mean=2,sd=1) 
gx.normal.samples <- dnorm(test.values,mean=0,sd=3)

plot(test.values, fx.normal.samples, ylim=c(0,1.1*max(fx.normal.samples)),
      type="l", xlab="test.values", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2, col = "red")
lines(test.values, gx.normal.samples, col = "blue")
legend("topright", c("f(x)", "g(x)"), col = c("red", "blue"), cex = 1, lty = 1)
```

#### Part D

First explore the expectation of $\mu_1$.
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)\right)+E\left(\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = \frac{1}{3}(-2)+\frac{2}{3}(2) = \frac{2}{3}$$

Next investigate the expectation of $\mu_2$.
$$\mu_2 = E\left(\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)^2\right)$$
To start, we can write:

$$E[X^2] = \sum_{i=1}^n w_i (E[X_i^2])$$
Then, we know,
$$Var[X] = E[X^2] - E[X]^2$$ 
Which, in this situation, let's us write:
$$\sigma^2_i = E[X_i^2] - \mu_i^2$$
And, finally,
$$E[X^2] = \sum_{i=1}^n w_i (\sigma^2_i + \mu_i^2)$$
Therefore:
$$E[X^2] = \frac{1}{3} (1 + 4) + \frac{2}{3}(1 + 4) = 5$$

Finally let us compute the expectation of $\theta$.
$$\theta = E\left(\exp\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)\right)$$
This paper explores the form of the multivariate lognormal's moment generating function [https://www.casact.org/pubs/forum/15spforum/Halliwell.pdf]. If I had more time I would have been interested to explore the derivation, but for now I will simply make use of Halliwell's result. 

$$E[\exp(X_i)] = \exp\left\{\mu_i + \Sigma_{ii} \frac{1}{2}\right\}$$
Therefore, we can calculate $\theta$ as follows:
$$\theta = \frac{1}{3}\exp(-2 + (1/2)) + \frac{2}{3}\exp(2 + (1/2)) \approx 8.196$$

```{r, imp.sampD}
#store the theoretically calculated values
mu1.theo <- 2/3 
mu2.theo <- 5 
theta.theo <- (1/3)*exp(-3/2) + (2/3)*exp(5/2)
```

#### Part E
See below for the outputs from the importance sampling algorithm without rejection control. The model is able to do pretty well!

```{r, imp.sampE}
#write functions to use as inputs to ImpSampler
logfx <- function(x){
  log((1/3)*dnorm(x,mean=-2,sd=1) + (2/3)*dnorm(x,mean=2,sd=1)) 
} 

loggx <- function(x){
  log(dnorm(x,mean=0,sd=3))
} 

gSampleFunc <- function(){
  rnorm(1,mean=0,sd = 3)
}

####importance sampling without rejection control
output.partE <- ImpSampler(5000, logfx, loggx, gSampleFunc)
# calculations estimate mu1, mu2, and theta
mu1 <- function(x){x}
mu2 <- function(x){x^2}
theta <- function(x){exp(x)}

# function to calculate the weighted average of each of the h(x) functions:
w.avg.func <- function(hx, imp.samp.list){ 
  return(sum(hx(imp.samp.list[[1]]) * exp(imp.samp.list[[2]]))/sum(exp(imp.samp.list[[2]])))
}

estimates <-  c(w.avg.func(mu1, output.partE), w.avg.func(mu2, output.partE), w.avg.func(theta, output.partE))
ess <- output.partE[[3]]

partE.df <- data.frame(c(mu1.theo,mu2.theo,theta.theo), estimates, c(ess,NA,NA))
partE.df <- round(partE.df, 3)
rownames(partE.df) <- c("mu1", "mu2", "theta")
colnames(partE.df) <- c("Theoretical Values", "Importance Sampling Estimates", "Estimated ESS")

kable(partE.df, "latex", caption = "Importance Sampling Without Rejection Control", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part F
Below is a table summarizing the importance sampling with varying rejection control. Below the table are the requested plots. As the rejection control constant increases, the acceptance rate decreases (i.e. the higher constant, the more strict we are about what we accept). This prefaces what we would expect to see from the ESS. It increases to its maximum value (5000) as the constant increases (the algorithm is in need of more samples as its restriction grow stronger). Lastly, the error plots show a sort of periodic behavior. I am unsure why this is happening, it is interesting to see that increasing the strength of our sample restrictions doesn't guarantee less error. This might simply be stochastic behavior given our model is already doing well. There is somewhat consistent decline in the error of $\mu_1$ as $c$ increases, so if one is particularly concerned with that estimate I might recommend using and increased $c$ value. Otherwise, it doesn't not appear to be too beneficial and could be left out for simplicity/interpretability.

```{r, imp.sampF}
####importance sampling with rejection control (1:10)
#initialize
acc.rates <- NULL
estimated.ESS <- NULL
mu1.errors <- NULL
mu2.errors <- NULL 
theta.errors <- NULL

#run through and store output with varying values for the rejection control as directed
for (i in 1:10) {
  output.partF <- ImpSampler(5000, logfx, loggx, gSampleFunc, rejectionControlConstant = i)
  acc.rates <- c(acc.rates, output.partF[[4]])
  estimated.ESS <- c(estimated.ESS, output.partF[[3]])
  mu1.errors <- c(mu1.errors, abs(w.avg.func(mu1, output.partF) - mu1.theo))
  mu2.errors <- c(mu2.errors, abs(w.avg.func(mu2, output.partF) - mu2.theo))
  theta.errors <- c(theta.errors, abs(w.avg.func(theta, output.partF) - theta.theo))
}

partF.df <- data.frame(round(rbind(mu1.errors, mu2.errors, theta.errors, acc.rates, estimated.ESS), 2))
rownames(partF.df) <- c("Mu1 Errors", "Mu2 Errors", "Theta Errors", "Acceptance Rates", "Estimated ESS")
colnames(partF.df) <- c("c=1","c=2","c=3","c=4","c=5","c=6","c=7","c=8","c=9","c=10")
kable(partF.df, "latex", caption = "Importance Sampling With Varying Rejection Control", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"), font_size = 8)
```

\newpage

```{r, imp.sampF plots, fig.width=3.5, fig.height=3.5, fig.align="default"}
plot(mu1.errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu1", ylab = "Absolute error")
plot(mu2.errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu2", ylab = "Absolute error")
plot(theta.errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating theta", ylab = "Absolute error" )

plot(acc.rates, ylim = c(0, 1), type = "l", xlab = "Rejection control constant", main = "Plot of acceptance rates", ylab = "Acceptance rate")
plot(estimated.ESS, ylim = c(0, 5000), type = "l", xlab = "Rejection control constant", main = "Plot of ESS", ylab = "ESS")
```

\newpage

## 2. Chapter 10 Question 5
#### Part A
Indeed, we are given the structure of the model from the problem statement. The information we have is as follows, where $J=10$:
$$p(y_j|\theta_j) \sim \text{Bin}(n_j,\theta_j), \text{ i.e.   } p(y|\theta,\alpha,\beta) = \prod_{j=1}^{10}{n_j \choose y_i}\theta_j^{y_i}(1-\theta_j)^{n_j-y_i}$$
$$p(\theta_j) \sim \text{logit}^{-1}(\alpha+\beta x_j)$$
$$\alpha \sim t_4(0, 2^2) \text{ and } \beta \sim t_4(0, 1^2)$$
$$x_j \sim U(0, 1) \text{ and } n_j \sim \text{Pois}(5)$$
I build the dataset of 10 samples from this model as directed (Table 1).
```{r, 10.5A}
#######################################
# 2. Chapter 10 Question 5
###10.5A
#build random dataset from the model
yis <- c(rep(NA, 10))
nis <- c(rep(NA, 10))
xis <- c(rep(NA, 10))
alphais <- c(rep(NA, 10))
betais <- c(rep(NA, 10))

set.seed(1)
for (i in 1:10) {
  alpha <- rmt(1,0,2,4)
  alphais[i] <- alpha
  
  beta <- rmt(1,0,1,4)
  betais[i] <- beta
  
  xi <- runif(1, min=0, max=1)
  xis[i] <- xi
  
  ni <- rpois(1,5)
  nis[i] <- ni
  
  p <- inv.logit(alpha + beta*xi)
  
  yis[i] <- rbinom(1, ni, p)
}

dataset <- cbind(yis,nis)
dataset.df <- t(data.frame(dataset))
kable(dataset.df, "latex", caption = "10.5 Sampled Data", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part B
Next, I used rejection sampling to acquire to get 1000 independent posterior draws from $(\alpha, \beta)$. The likelihood follows:
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta)\prod_{i=1}^k p(y_i|\alpha,\beta,n_i,x_i)$$
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta) \prod_{i=1}^k [\text{logit}^{-1}(\alpha+\beta x_i)]^{y_i}[1-\text{logit}^{-1}(\alpha+\beta x_i)]^{n_i-y_i}$$
For simplitcity's sake we will let the prior distribution equal the inverse of the normalization constants (is this what you said, Jerson? I can't quite remember if this was what you recommended for forgoeing the specification of a prior but I can't think of what else you might have said...). Given that we also have no knowledge that would given us reason to choose a particular prior, this is as appropriate a choice as any. Therefore, I calculated the posterior as follows:
$$q(\alpha,\beta|y,n,x) \propto \prod_{i=1}^k [\text{logit}^{-1}(\alpha+\beta x_i)]^{y_i}[1-\text{logit}^{-1}(\alpha+\beta x_i)]^{n_i-y_i}$$


First, I display the contours of the joint posterior distribution of $(\alpha, \beta)$ for your reference. The following plot shows the draws from the rejection sampling.

```{r, 10.5B}
###10.5B
library(LearnBayes)
#posterior function
logitBin = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  N = length(xis)
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*xis)) + 
    (n-y)*log((1-inv.logit(alpha + beta*xis)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  post = loglikelihood #+ logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 30
x0 = seq(-5, 5, len = ng)
y0 = seq(-15, 10, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin, data = dataset)
Z = Z - max(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Contour plot of joint posterior")

####Rejection sampling
#find the mode
tpar=LearnBayes::laplace(logitBin,array(c(3,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar) {
	data=data
	d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,
	log=TRUE)
	return(d)
}

start=array(c(3,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT, start, data=dataset, tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

rejectsamp=function(logf,tpar,dmax,n,data){
	theta=rmt(n,mean=c(tpar$mode),S=2*tpar$var,df=4)
	lf=apply(theta,1,logf,data=data)
	lg=dmt(theta,mean=c(tpar$mode),S=2*tpar$var,df=4,log=TRUE)
	prob=exp(lf-lg-dmax)
	return(theta[runif(n)<prob,])
}

theta=rejectsamp(logitBin,tpar,logitBinT(fit1$mode,dataset,tpar),2000,dataset)
#dim(theta)
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Rejection Sampling",
        drawlabels = F)
points(theta[,1],theta[,2])
```

#### Part C

Here we use the Laplace function to estimate the approximate the posterior density for $(\alpha,\beta)$ with a normal centered at the posterior mode with covariance matrix fit to the curvature at the mode. See below for the estimates:

```{r, 10.5C}
###10.5C
# build normal approximation
tpar=LearnBayes::laplace(logitBin,array(c(-1,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar) {
  data=data
  d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
  return(d)
}
start=array(c(-1,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT,start,data=dataset,tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

dataset.df10.5C1 <- data.frame(fit1$mode)
colnames(dataset.df10.5C1) <- c("alpha", "beta")
rownames(dataset.df10.5C1) <- c("")
kable(dataset.df10.5C1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

dataset.df10.5C2 <- data.frame(fit1$var)
colnames(dataset.df10.5C2) <- c("alpha", "beta")
rownames(dataset.df10.5C2) <- c("alpha", "beta")
kable(dataset.df10.5C2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part D
Using the calculated posterior mode and covariance matrix to build two-dimensional $t_4$ distributions, I employed importance sampling to estimate $E(\alpha|y)$ and $E(\beta|y)$. First, see the samples displayed on the contour plot. The table below that displayes the distribution of the estimates for $(\alpha,\beta)$. The means of the 1,000 samples were calculated as 0.205 for $\alpha$ and -2.01 for $\beta$.

```{r, 10.5D}
###10.5D
##importance sampling using normal centered at the posterior mode with covariance 
    #matrix fit to the curvature at the mode
theta = rmt(1000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin,data=dataset)
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
wt = exp(lf - lp)
probs=wt/sum(wt, na.rm = T)
probs[is.na(probs)] <- 0
theta.S=theta[sample(1:1000,size=1000,prob=probs,replace=TRUE),]

contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2, 
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Importance Sampling",
        drawlabels = F)
points(theta.S[,1],theta.S[,2])

#estimates
dataset.df10.5D <- t(data.frame(apply(theta.S,2,summary)))
rownames(dataset.df10.5D) <- c("alpha", "beta")
kable(dataset.df10.5D, "latex", caption = "10.5 Estimates for Expectations", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part E
I calculated the effective sample size to be 918.74 using the following equation from BDA3 (pg266, 10.4):

$$S_{eff} = \frac{1}{\sum_{s=1}^S(\tilde{w}(\theta^s))^2}$$
where $\tilde{w}(\theta^s)$ is simply the standardized weights. This is slightly below the 1000 samples we were instructed to pull from our distribution.

```{r, 10.5E}
###10.5E
normalize = function(x){return(x/sum(x))}
seff <- 1/sum(normalize(wt)^2)
```


## 3. Chapter 10 Question 8
#### Part A

First, I set out to recreate the approximate posterior distribution of the bioassay example in Section 3.7. The model has the same structure as was written out for Problem 10.7. I will forgoe re-writing it here. My first plot below is my recreation of Figure 3.3b to confirm that I am recreating the distribution correctly. After sampling 10,000 from the approximate distribution, I resampled without replacement $k = 1000$ samples. The plot of those draws are shown on the next plot.

```{r, 108A}
#######################################
# 3. Chapter 10 Question 8
###10.8A
# load data
x108 <- c(-0.86,-0.30,-0.05,0.74) 
n108 <- rep(5,length(x108))
y108 <- c(0,1,3,5)
dataset108<-cbind(y108,n108)

#posterior function
logitBin108 = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*x108)) + 
    (n108-y)*log((1-inv.logit(alpha + beta*x108)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  logPrior <- 0 + 0 #non informative 
  post = loglikelihood + logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 50
#grid suggestion from book
x0 = seq(-2, 7, len = ng)
y0 = seq(-2, 35, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin108, data = dataset108)
Z = exp(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Contour Plots to Compare to Figure 3.3b",
        drawlabels = F)

# build normal approximation
tpar=LearnBayes::laplace(logitBin108,array(c(1,10),c(1,2)),data=dataset108)
betabinT108=function(theta,data,tpar) {
	data=data
	d=logitBin108(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
	return(d)
}
start=array(c(1,10),c(1,2))
fit1=LearnBayes::laplace(betabinT108,start,data=dataset108, tpar=tpar)
#betabinT108(fit1$mode,dataset108,tpar)

#importance resampling SIR without replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=F),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling Without Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```

#### Part B
See below for my first attempt at displaying the distribution of the simulated importance ratios. 

```{r}
###10.8B
hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios with Outliers")
```

For a more representative display of the simulated importance ratios, I remove multiple outliers from the upper end of the distribution. The resulting distribution is displayed below. The importance ratios appear to ratios vary as we would hope! 

```{r}
#remove outliers
for(i in 1:40){
  wt<-wt[wt < max(wt)]
}

hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios without Outliers")
```

#### Part C

Below I display the results from doing importance resampling with replacement. There does not appear to be much difference between the samples drawn when replacing that in Part A where there was none. This is an indicator that the importance weights are moderate, whichi is in line with what we saw in Part B. In situations like these, it would make sense that sampling with and without replacement gives similar results (as compared to a situation where there are a few large weights and many small weights and sampling with replacement would pick the same few values repeatedly, leading to a different set that a more conservative without replacement approach).

```{r, 108C}
###10.8C
#importance resampling SIR WITH replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=T),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling With Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```




## 4. Chapter 11 Question 2
Following up on questions 10.7 and 10.8, this problem encourages us to try sampling using the the Metropolis algorithm. First, I defined my starting points and my jumping rule. I used a normal approximation to construct estimates for these parameters (Table 5).


```{r, 112}
#######################################
# 4. Chapter 11 Question 2
###11.2
#initializations
sims <- 1000
theta <-  matrix(0, nrow = sims, ncol = 2) 
r.all <- rep(0,sims) 

#starting positions found from normal approximation
theta[1,] <- c(0.847,7.749)

dataset.df11.2.1 <- t(data.frame(theta[1,]))
colnames(dataset.df11.2.1) <- c("alpha", "beta")
rownames(dataset.df11.2.1) <- c("")
kable(dataset.df11.2.1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#jumping function
jumping <- function(theta, scale=.1) rmnorm(1, mean = c(tpar$mode), varcov = matrix(tpar$var,2,2))

dataset.df11.2.2 <- data.frame(matrix(tpar$var,2,2))
colnames(dataset.df11.2.2) <- c("alpha", "beta")
rownames(dataset.df11.2.2) <- c("alpha", "beta")
kable(dataset.df11.2.2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

Now, we use the metropolis algorithm to pull a set of samples. These samples are displayed below. They seem to be an improvement over the importance sampling attempts.

```{r, 1121}
#metropolis algorithm
for(i in 2:sims) {
  theta.new = jumping(theta[i-1,], .1)
  uu = runif(1)
  r <- min(exp(logitBin108(theta.new, dataset108) - logitBin108(theta[i-1,], dataset108)), 1)
  if (uu < r) {
    theta[i,] <- theta.new
  }else{
    theta[i,] = theta[i-1,]
  }
}

#samples/walks
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        drawlabels = F)
points(theta[,1],theta[,2], pch='.')
```

Lastly, we are instructed to check if we have approximated convergence. Below I display the values of alpha and beta as we progress through the iterations. They look as we would expect and hope! There are few moments where they seem to get stuck briefly, but otherwise they seem to perform as we would hope.

```{r, 1122}
# plots for convergence? / investigation / display
firstChain <- c(rep(NA, sims))
for (i in 1:sims) {
  firstChain[i] <- exp(logitBin108(theta[i,],dataset108))
}
# probability
#plot(seq(1:sims),firstChain,type="l",xlab="Iteration",
    #ylab="posterior prob",main="post convergence?")
#alpha
plot(seq(1:sims),theta[,1],type="l",xlab="Iteration",
     ylab="Sampled Alpha",main="Alpha Convergence")
#beta
plot(seq(1:sims),theta[,2],type="l",xlab="Iteration",
     ylab="Sampled Beta",main="Beta Convergence")
```


## 5. Chapter 11 Question 3

The question asks us to compare the results from using three types of models on the given data: the hierarchical normal model, the seperate model, and the pooled model. To construct the hierarchical model I used the given conditional densities in the BDA3 text (equations 11.9-11.7). I will not re-write them here to save time, but they are coded out very clearly in the code appendix. The distributions of the output from the hierarchical model are displayed below. I see that the estimates are slightly off from what we might hope. I have combed through my code and can't seem to find what might be causing the discrepancy. I did take the time to try and code my gibbs sampler myself, it's possible that something is going slightly wrong in there?

```{r, 113}
#######################################
# 5. Chapter 11 Question 3
###11.3
#load in data
dataset113 <-  data.frame(machineno = rep(1:6, each=5), 
                          ments = c(83, 92, 92, 46, 67,
                                    117, 109, 114, 104, 87,
                                    101, 93, 92, 86, 67,
                                    105, 119, 116, 102, 116,
                                    79, 97, 103, 79, 92,
                                    57, 92, 104, 77, 100))
J <- length(unique(dataset113$machineno))
n <- length(dataset113$ments)

#starting points
# from text "obtain 10 starting points for the simulations by drawing thetaj 
    #independently in this way for each group"

set.seed(142857)
theta.start <- sapply(1:6,function(x) sample(dataset113$ments[dataset113$machineno==x], 10, replace=TRUE))
mu.start <- apply(theta.start, 2, mean)
sigma.start <- sqrt(apply(theta.start, 2, var))

# ____________________________________________________________________________________________________________________

#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}
#sigma (step 3)
sigma.post.sample <-  function(theta) {
  #first find sigma hat
  sigma.hat.func <- function(theta) {
    sigma.hat <- sapply(1:6, function(x) (dataset113$ments[dataset113$machineno==x] - theta[x])^2)  
    sigma.hat <- (1/n) * sum(unlist(sigma.hat))
    return(sigma.hat)
  }
  #next find conditional sigma
  sigma.hat <- sigma.hat.func(theta) 
  sigma.cond <- rinvchisq(1,n,sigma.hat)
  return(sigma.cond)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma)+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6", 
                               "mu", "sigma2", "tau2")
  
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,9] <- tau.post.sample(sample.i)
  param.storage[1,8] <- sigma.post.sample(sample.i)
  param.storage[1,7] <- mu.post.sample(sample.i,param.storage[1,9])#param.storage[1,9])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,7], param.storage[i-1,8], param.storage[i-1,9])
    param.storage[i,9] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,8] <- sigma.post.sample(param.storage[i,1:6])
    param.storage[i,7] <- mu.post.sample(param.storage[i,1:6], param.storage[i,9])
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,8:9] <- sqrt(gibbs.output[,8:9])

#summary table
df.11.3.output <- data.frame(t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975)))))
colnames(df.11.3.output) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(df.11.3.output, "latex", caption = "Hierarchical Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

First, they ask us to report (i) the posterior distribution of the mean of the quality measurements of the sixth machine for each of the three models. I referred to the construction of the hierarchical distribution above. I drew from this model by drawing samples from: $N(\theta_6,|\sigma^2)$. To build the "seperate" model, I simply calculated the mean and variance of the sixth machine measurements without taking into account any of the other machine values and then used them as the parameters of the normal distribution, i.e. $N(\bar{y_{6.}},V^{sep}_6)$. Finally, to build the "pooled" model, I simply calculated the mean across all of the samples and the variance of the sixth machine measurements compared to the overall mean, i.e. $N(\bar{y_{..}},V^{pooled}_6)$. These distributions are displayed below. Notably, they appear as we might expect: the hierarchical has the highest density around the sampled mean of the sixth machine, the pooled distribution is pulled higher by the other machine values, and the seperated has the widest spread.

```{r, pooled/sep calcs, i}
#measures for theta 6 for seperate model
y.bar.6.dot <- mean(dataset113$ments[dataset113$machineno==6]) 
var.sep.6 <- var(dataset113$ments[dataset113$machineno==6])
theta.sep.6 <- rnorm(sims, y.bar.6.dot, sqrt(var.sep.6))

#measures for theta 6 for pooled model
y.bar.dot.dot <- mean(dataset113$ments)
var.pooled <- sum((dataset113$ments[dataset113$machineno==6] - mean(dataset113$ments))^2) / 
  (length(dataset113$ments) - 1)
theta.pooled.6 <- rnorm(sims, y.bar.dot.dot, sqrt(var.pooled))

# posterior distribution of the mean of the quality measurements of the sixth machine
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="(i) Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))
```

Next, they ask us to report (ii) the predictive distribution for another quality measurement of the sixth machine. I was a bit unsure what they were asking for here, but  I decided to simply compare how each of the three models are performing by calculating p-values for a text statistic of the mean of the sixth machine. The observed values is displayed with a black line on the plot below. The p-values are displayed below the graph for your reference. They suggest that the hierarchical model and separate model will predict the mean of the sixth machine best.

```{r, ii}
# predictive distribution for another quality measurement of the sixth machine (maximum?)
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="(ii) Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
lines(x=c(y.bar.6.dot,y.bar.6.dot),y=c(-1,1), col="black")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))

pvalmean1 <- length(which(gibbs.output[,"theta6"] > mean(y.bar.6.dot))) / 
  length(gibbs.output[,"theta6"])

pvalmean2 <- length(which(theta.pooled.6[seq(sims/2+1, sims, 1)] > mean(y.bar.6.dot))) / 
  length(theta.pooled.6[seq(sims/2+1, sims, 1)])

pvalmean3 <- length(which(theta.sep.6[seq(sims/2+1, sims, 1)] > mean(y.bar.6.dot))) / 
  length(theta.sep.6[seq(sims/2+1, sims, 1)])

df.11.3.pvals <- data.frame(pvalmean1, pvalmean2, pvalmean3)
colnames(df.11.3.pvals) <- c("Hierarchical", "Pooled", "Seperate")
rownames(df.11.3.pvals) <- c("P-Value")
kable(df.11.3.pvals, "latex", caption = "Posterior Predictive Check", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

Lastly, they ask us to report (iii) the posterior distribution of the mean of the quality measurements of the seventh machine. As we are given no information about the seventh machine, such as if it shares any characteristics with one of the sixth machines in our dataset, we take a cautious approach and create distributions using the grand means of the distributions. In practice, this means we pull from the following distributions: Hierarchical: $N(\mu,\tau)$ and Pooled: $N(\bar{y}_{..},V_{pooled})$. We would not be able use the Seperate model for this prediction as the seperate model only claims to provide information about the six machines in our data set. The distributions of the Hierarchical and Pooled models are displayed below. We can see that the hierachical model put the majority of its predictions in a much narrower interval than the pooled, as we would expect.

```{r, iii}

#measures for machine 7 for pooled model
y.bar.dot.dot <- mean(dataset113$ments)
var.pooled <- var(dataset113$ments)
#sum((dataset113$ments[dataset113$machineno==6] - mean(dataset113$ments))^2) / 
 # (length(dataset113$ments) - 1)
theta.pooled.7 <- rnorm(sims, y.bar.dot.dot, sqrt(var.pooled))

#hierarchical
theta.hier.7 <- rnorm(sims, gibbs.output[,"mu"], sqrt(gibbs.output[,"sigma2"]))

# posterior distribution of the mean of the quality measurements of the potential seventh machine
plot(density(theta.hier.7), col="red", xlab="Mean Measure", 
    ylab="Density", main="(iii) Mean of Potential Machine 7")
lines(density(theta.pooled.7[seq(sims/2+1, sims, 1)]), col="blue")
legend("topright",col = c("red","blue"), 
       legend=c("hierarchical","pooled"), 
       lty = c(1,1,1)) 
```


## 6* Chapter 11 Question 4

Derivation of $\sigma_j$ form here. 

Explanation of use of $\sigma_0$ grid here and concerns. Derivation of $\sigma_0$ form?

```{r, 114}
#######################################
# 6. Chapter 11 Question 4
###11.4
#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}

# change sigma function
sigma.post.sample.p2 <- function(theta, mu) {
  sigma2.star <- NULL
  for(j in 1:J)	{
    k <- 5#length( y )
    nu.k <- 1 + k
    sigma.0.grid <- seq(0.25, 30, 0.25)
    sigma2.star.vals<-NULL
    probs <- NULL
    for (i in 1:length(sigma.0.grid)) {
      v <- mean((theta[j] - mu)^2)
      sigma2.k.option <- (1 * sigma.0.grid[i] + k*v) / (1 + k) #nu.0=1
      sigma2.star.vals[i] <- rinvchisq(1, nu.k, sigma2.k.option)
      probs[i] <- dinvchisq(sigma2.star.vals, nu.k, sigma2.k.option)
    }
    #print(mean(probs))
    sigma2.star[j] <- sample(sigma2.star.vals, 1, prob=probs)#rinvchisq(1, nu.k, sigma2.k)  
  }
  return(sigma2.star)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma[j])+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma[j]) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 14
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6",
                               "sigma1", "sigma2", "sigma3", "sigma4", "sigma5", "sigma6",
                               "tau2","mu")
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,13] <- tau.post.sample(sample.i)
  param.storage[1,14] <- mu.post.sample(sample.i,param.storage[1,13])
  param.storage[1,7:12] <- sigma.post.sample.p2(sample.i,param.storage[1,14])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,14], param.storage[i-1,7:12], param.storage[i-1,13])
    param.storage[i,13] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,14] <- mu.post.sample(param.storage[i,1:6], param.storage[i,13])
    param.storage[i,7:12] <- sigma.post.sample.p2(param.storage[i,1:6], param.storage[i,14])
    
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,7:13] <- sqrt(gibbs.output[,7:13])

#summary table
df.11.4.output <- data.frame(t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975)))))
colnames(df.11.4.output) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(df.11.4.output, "latex", caption = "Hierarchical Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```


\newpage

## 7. Chapter 13 Question 5
Estimating the number of unseen species (see Fisher, Corbet, and Williams, 1943, Efron and Thisted, 1976, and Seber, 1992): suppose that during an animal trapping expedition the number of times an animal from species i is caught is $x_i \sim Poisson(\lambda_i)$. For parts (a)–(d) of this problem, assume a $Gamma(\alpha,\beta)$ prior distribution for the $\lambda_i$’s, with a uniform hyperprior distribution on $(\alpha,\beta)$. The only observed data are $y_k$, the number of species observed exactly k times during a trapping expedition, for $k = 1, 2, 3,...$

#### Part A
Write the distribution $p(x_i|\alpha,\beta)$.

Recall that we are given the following:

$$x_i \sim Poisson(\lambda_i)$$ 
$$\lambda \sim Gamma(\alpha,\beta)$$
$$p(\alpha, \beta) \propto 1$$
Therefore, we can start by writing the joint density as follows:

$$p(x_i, \lambda_i) = \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda_i^{\alpha-1}e^{-\beta\lambda_i} \frac{e^{-\lambda_i}\lambda_i^{x_i}}{x_i!}$$
We can integrate over $\lambda_i$ to find the unconditional distribution of $x_i$.
$$p(x_i) = \int_0^{\infty} \frac{\beta^\alpha}{\Gamma(\alpha)} \lambda_i^{\alpha-1}e^{-\beta\lambda_i} \frac{e^{-\lambda_i}\lambda_i^{x_i}}{x_i!} d\lambda_i$$
$$p(x_i) = \frac{\beta^\alpha}{x_i! \Gamma(\alpha)} \int_0^{\infty}  \lambda_i^{x_i+\alpha-1}e^{-(\beta+1)\lambda_i} d\lambda_i$$
$$p(x_i) = \frac{\beta^\alpha}{x_i! \Gamma(\alpha)} \frac{\Gamma(x_i+\alpha)}{(\beta+1)^{x_i+\alpha}} \int_0^{\infty}\frac{(\beta+1)^{x_i+\alpha}}{\Gamma(x_i+\alpha)} \lambda_i^{x_i+\alpha-1}e^{-(\beta+1)\lambda_i} d\lambda_i$$
We have contructed the above expression such that the inegrand is of the form of the PDF of the Gamma distribution, so the integral, in fact, simply equals 1. Now,
$$p(x_i) = \frac{\beta^\alpha}{x_i! \Gamma(\alpha)} \frac{\Gamma(x_i+\alpha)}{(\beta+1)^{x_i+\alpha}}$$
$$p(x_i) = \frac{\Gamma(x_i+\alpha)}{x_i!\Gamma(\alpha)} \left(\frac{\beta}{\beta+1}\right)^{\alpha} \left(\frac{1}{\beta+1}\right)^{x_i} $$

We note that this is a form of the negative binomial distribution! 

If we transform $\alpha$ and $\beta$ such that $p =1/(\alpha+1)$ and $r=\beta$, we reach the more recognizable form of :
$$p(x_i) = \frac{\Gamma(x_i+r)}{x_i!\Gamma(r)} (1-p)^r (p)^{x_i} $$
I am going to use this form for the remainder of the problem as it is more intuitive for me.

#### Part B
Use the distribution of $x_i$ to derive a multinomial distribution for y given that there are a total of N species.

We are given that $\sum_{k=1}^r y_k = N$. We should also define the varying probabilities of seeing each species. Let us define $p_i$ probabilities where $0<p_i<1$ and $\sum_{k=0}^r p_k = 1$, i.e.  $p_0 = 1- \sum_{k=1}^r p_k$. Lastly, we let $y_0$ represent the number of failures ("unseen species?") before the search is stopped. Then we can write:

$$p(y_k) = \frac{\Gamma(\sum_{k=1}^r y_k+y_0)}{y_k!\Gamma(y_0)} (p_0)^{y_0} \prod_{k=1}^r (p_k)^{y_k} $$
I used Gall, 2006 [https://www.sciencedirect.com/science/article/pii/S0167715205003639#bib3] for guidance here.
$$p(y_k) = \frac{\Gamma(496+y_0)}{y_k!\Gamma(y_0)} (p_0)^{y_0} \prod_{k=1}^{24} (p_k)^{y_k} $$
We can re-write this using $\alpha$'s and $\beta$'s:

$$p(y_k) = \frac{\Gamma(496+\beta)}{y_k!\Gamma(\beta)} \left(\frac{\alpha_i}{\alpha_i+1}\right)^{\beta} \prod_{k=1}^{24} \left(\frac{1}{\alpha_i+1}\right)^{y_k} $$

#### Part C
Suppose that we are given y = (118, 74, 44, 24, 29, 22, 20, 14, 20, 15, 12, 14, 6, 12, 6, 9, 9, 6, 10, 10, 11, 5, 3, 3), so that 118 species were observed only once, 74 species were observed twice, and so forth, with a total of 496 species observed and 3266 animals caught. Write down the likelihood for y using the multinomial distribution with 24 cells (ignoring unseen species). Use any method to find the mode of $(\alpha,\beta)$ and an approximate second derivative matrix.

We know:
$$\sum_{k=1}^{24} y_k = 496$$. Therefore, we can write the likelihood of $y$ as:

$$L(\alpha,\beta,y_k) \propto \prod_{k=1}^{24}\left[\frac{\Gamma(\alpha+x_k)}{x_k!\Gamma(\alpha)}\frac{\beta^\alpha}{(\beta+1)^{\alpha+x_k}}\right]^{y_k}$$

$$L(\alpha,\beta,y_k) \propto \left[\frac{\Gamma(\alpha+1)}{1!\Gamma(\alpha)}\frac{\beta^\alpha}{(\beta+1)^{\alpha+1}}\right]^{118} \left[\frac{\Gamma(\alpha+2)}{2!\Gamma(\alpha)}\frac{\beta^\alpha}{(\beta+1)^{\alpha+2}}\right]^{74}...\left[\frac{\Gamma(\alpha+24)}{24!\Gamma(\alpha)}\frac{\beta^\alpha}{(\beta+1)^{\alpha+24}}\right]^{3}$$
Now, to find the mode of $(\alpha,\beta)$, I will use a normal approximation with the "laplace" function.

```{r, Chapter 13 Question 5, Part C}
library(MGLM)

yks = c(118, 74, 44, 24, 29, 22, 20, 14, 20, 15, 12, 14, 6, 12, 6, 9, 9, 6, 10, 10, 11, 5, 3, 3)
xks = c(1:24)
dataset13.5 <- cbind(yks,xks)

species.observed <- sum(yks)
animals.caught <- sum(yks * c(1:24))
probs <- (1/species.observed) * c(1:24)

###10.5B
#posterior function
negmultbin = function (theta, data) {
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  x = data[,2]
  #N = length(xis)
  prelikelihood <- function(y,alpha,beta) y*log( (gamma(alpha+x)/(factorial(x)*gamma(alpha))) * ((beta^alpha)/((beta+1)^(alpha+x))) )
    #y*log(inv.logit(alpha + beta*xis)) + (n-y)*log((1-inv.logit(alpha + beta*xis)))
  loglikelihood <- sum(prelikelihood(y, alpha, beta))
  post = loglikelihood #+ logPrior
  return(post)
}

#find mode
tpar=LearnBayes::laplace(negmultbin,array(c(1,0.1),c(1,2)),data=dataset13.5)

#build contours
ng = 30
x0 = seq(0, 5, len = ng)
y0 = seq(0, 15, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, negmultbin, data = dataset13.5)
Z = exp(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(-10000, -7000, -5000, -1000, -800, -30, -20, -10, -5, -1, -0.1), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Contour plot of joint posterior")

#output desired information
dataset.df13.5D1 <- data.frame(tpar$mode)
colnames(dataset.df13.5D1) <- c("alpha", "beta")
rownames(dataset.df13.5D1) <- c("")
kable(dataset.df13.5D1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

dataset.df13.5D2 <- data.frame(tpar$var)
colnames(dataset.df13.5D2) <- c("alpha", "beta")
rownames(dataset.df13.5D2) <- c("alpha", "beta")
kable(dataset.df13.5D2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part D
Derive an estimate and approximate 95% posterior interval for the number of additional species that would be observed if 10,000 more animals were caught.

#### Part E
Evaluate the fit of the model to the data using appropriate posterior predictive checks.

#### Part F
Discuss the sensitivity of the inference in (d) to each of the model assumptions.










\newpage
## 8* Generate / simulate n = 120 observations from a three component mixture 
We are instructed to simulate n = 120 observations from a three component mixture Gaussian model where $(p_1, p_2, p_3) = (0.1, 0.3, 0.6)$ and $(\mu_1, \sigma_1^2) = (0,1)$, $(\mu_2, \sigma_21^2) = (-2,2)$, and $(\mu_3, \sigma_3^2) = (3,16)$. See below for the mixture model's distribution where the three means are displayed with black lines. I use k-means clustering algorithm to build a set of labels to initialize my EM algorithm.

```{r, Prep}
#######################################
# 8. Generate / simulate n = 120 observations from a three component mixture 
set.seed(100)
#investigate model and pull 120 observations as directed
y <- seq(-10,15,0.1)
#dens <- function (x, theta, stnd){dnorm (x, theta, sqrt(1000*theta*(1-theta)))}
dens.mix <- 0.1*dnorm(y,0,1) + 0.3*dnorm(y,-2,sqrt(2)) + 0.6*dnorm(y,3,sqrt(16))
plot (y, dens.mix, ylim=c(0,1.1*max(dens.mix)),
      type="l", xlab="", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2,
      main="Gaussian Mixture Model")
lines(x=c(0,0), y=c(0,0.5))
lines(x=c(-2,-2), y=c(0,0.5))
lines(x=c(3,3), y=c(0,0.5))

vals.mix <- c(rnorm(120*0.1,0,1), rnorm(120*0.3,-2,sqrt(2)), rnorm(0.6*120,3,sqrt(16)))
labels <- c(rep(1, 12), rep(2, 36), rep(3, 72))
vals.mix <- data.frame(cbind(vals.mix, labels))

#use k means to initialize
vals.kmeans <- kmeans(vals.mix$vals.mix, 3) 
vals.kmeans.cluster <- vals.kmeans$cluster
vals.df <- data.frame(x = vals.mix$vals.mix, 
                      cluster = vals.kmeans.cluster)

#vals.df %>%
 # dplyr::mutate(num = row_number()) %>%
  #ggplot(aes(y = num, x = x, color = factor(cluster))) +
  #geom_point() +
  #ylab("Values") +
  #ylab("Data Point Number") +
  #scale_color_discrete(name = "Cluster") +
  #ggtitle("K-means Clustering")

vals.summary.df <- vals.df %>%
  dplyr::group_by(cluster) %>%
  dplyr::summarize(mu = mean(x), variance = var(x), std = sd(x), size = n()) %>%
  dplyr::mutate(p = size / sum(size))
```

#### Part A
First, we implement an EM algorithm to find the MLEs for $p_i, \mu_i,$ and $\sigma^2_i$ for $i\in \{1,2,3\}$. I coded three functions: the e-step, the m-step, and a function to loop/iterate between the two. All of the code can be found in the appendix. The output of this algorithm is displayed below both in the table and the figure. They obviously fit the data well but do not end up at the values that we used to build the mixture model, rather they pretty much stay at the values built by the k-means labels. I will discuss this further in Part C. 

```{r, 8A}
###Problem 8 Part A
#e step
e_step <- function(x, mu.vector, sd.vector, p.vector) {
  comp1.prod <- dnorm(x, mu.vector[1], sd.vector[1]) * p.vector[1]
  comp2.prod <- dnorm(x, mu.vector[2], sd.vector[2]) * p.vector[2]
  comp3.prod <- dnorm(x, mu.vector[3], sd.vector[3]) * p.vector[3]
  sum.of.comps <- comp1.prod + comp2.prod + comp3.prod
  comp1.post <- comp1.prod / sum.of.comps
  comp2.post <- comp2.prod / sum.of.comps
  comp3.post <- comp3.prod / sum.of.comps

  sum.of.comps.log.sum <- sum(log(sum.of.comps))

  list("loglik" = sum.of.comps.log.sum,
       "posterior.df" = cbind(comp1.post, comp2.post, comp3.post))
}

#m step
m_step <- function(x, posterior.df) {
  comp1.n <- sum(posterior.df[,1])
  comp2.n <- sum(posterior.df[,2])
  comp3.n <- sum(posterior.df[,3])
  
  comp1.mu <- 1/comp1.n * sum(posterior.df[,1] * x)
  comp2.mu <- 1/comp2.n * sum(posterior.df[,2] * x)
  comp3.mu <- 1/comp3.n * sum(posterior.df[,3] * x)

  comp1.var <- sum(posterior.df[,1] * (x - comp1.mu)^2) * 1/comp1.n
  comp2.var <- sum(posterior.df[,2] * (x - comp2.mu)^2) * 1/comp2.n
  comp3.var <- sum(posterior.df[,3] * (x - comp3.mu)^2) * 1/comp3.n

  comp1.p <- comp1.n / length(x)
  comp2.p <- comp2.n / length(x)
  comp3.p <- comp3.n / length(x)
  
  list("mu" = c(comp1.mu, comp2.mu, comp3.mu),
       "var" = c(comp1.var, comp2.var, comp3.var),
       "p" = c(comp1.p, comp2.p, comp3.p))
}

#loop function
for (i in 1:2000) {
  if (i == 1) {
    # Initialization
    e.step <- e_step(vals.mix$vals.mix, 
                     vals.summary.df[["mu"]], 
                     vals.summary.df[["std"]],
                     vals.summary.df[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    cur.loglik <- e.step[["loglik"]]
    loglik.vector <- e.step[["loglik"]]
    
  } else {
    # Repeat E and M steps till convergence
    e.step <- e_step(vals.mix$vals.mix, m.step[["mu"]], 
                     sqrt(m.step[["var"]]), 
                     m.step[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    loglik.vector <- c(loglik.vector, 
                       e.step[["loglik"]])
    loglik.diff <- abs((cur.loglik - e.step[["loglik"]]))
    if(loglik.diff < 1e-6) {
      break
    } else {
      cur.loglik <- e.step[["loglik"]]
    }
  }
}

#loglik.vector
m.stepdf <- t(data.frame(m.step))
colnames(m.stepdf) <- c("Gaussian 1", "Gaussian 2", "Gaussian 3")
kable(m.stepdf, "latex", caption = "EM Algorithm MLEs", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))


plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[1], sqrt(m.step$var[1]), 
                           lam = m.step$p[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[2], sqrt(m.step$var[2]), 
                            lam = m.step$p[2]),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[3], sqrt(m.step$var[3]), 
                            lam = m.step$p[3]),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("EM Algorithm Fit")
```

#### Part B
Next, we implement an the MCM Gibbs algorithm to find the posterior distributions for $p_i, \mu_i,$ and $\sigma^2_i$ for $i\in \{1,2,3\}$. All of the code can again be found in the appendix. The output of this algorithm is displayed below both in the table and the figure. The Gibbs algorithm seems to do just as well as the EM algorithm. Interestingly, it gets similarly stuck in the same wrong spot. Notably, I used completely uninformative priors (compared to above where I used the k-means clustering labels). If I had more time I would try out other priors and possibly include a function to add more variability to steps' movements.

ADD IN SIGMA PROGRESSION

```{r, 8B}
###Problem 8 Part B
# mixed gaussian model functions

normalize = function(x){return(x/sum(x))}

sample_z = function(x,pi,mu){
  dmat = outer(mu,x,"-") # k by n matrix, d_kj =(mu_k - x_j)
  p.z.given.x = as.vector(pi) * dnorm(dmat,0,1) 
  p.z.given.x = apply(p.z.given.x,2,normalize) # normalize columns
  z = rep(0, length(x))
  for(i in 1:length(z)){
    z[i] = sample(1:length(pi), size=1,prob=p.z.given.x[,i],replace=TRUE)
  }
  return(z)
}

sample.pi.post = function(z,k) {
  counts = colSums(outer(z,1:k,FUN="=="))
  pi = LearnBayes::rdirichlet(1,counts+1)
  return(pi)
}

sample_mu = function(x, z, k, prior){
  df = data.frame(x=x,z=z)
  mu = rep(0,k)
  for(i in 1:k){
    sample.size = sum(z==i)
    sample.mean = ifelse(sample.size==0,0,mean(x[z==i]))
    post.prec = sample.size+prior$prec
    post.mean = (prior$mean * prior$prec + sample.mean * sample.size)/post.prec
    mu[i] = rnorm(1,post.mean,sqrt(1/post.prec))
  }
  return(mu)
}

#gibbs sampler function
sims <- 1000

gibbs.sampler.8 <- function(x, k, muprior = list(mean=0,prec=0.1)) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, nrow=sims, ncol=no.params)
  colnames(param.storage) <- c("mu1", "mu2", "mu3",
                               "sigma1", "sigma2", "sigma3",
                               "p1", "p2", "p3")
  z.storage <- matrix(NA, nrow=sims, ncol=length(vals.mix$vals.mix))
  
  #starting values for theta, tau, sigma and mu (calculated from our sample build earlier)
  param.storage[1,1:3] <- c(0,0,0)#vals.summary.df$mu
  param.storage[1,4:6] <- vals.summary.df$variance
  param.storage[1,7:9] <- c(1/3,1/3,1/3)#vals.summary.df$p
  z.storage[1,] <- rep(c(1,2,3),40)#vals.kmeans.cluster
  
  #interations
  for (i in 2:sims) {
    z.storage[i,] = sample_z(x,param.storage[i-1,7:9],param.storage[i-1,1:3])
    param.storage[i,7:9] = sample.pi.post(z.storage[i,],k)
    param.storage[i,1:3] = sample_mu(x,z.storage[i,],k,muprior)
  }
return(param.storage)
return(z.storage)
}

gibbs.output.8 <- data.frame(gibbs.sampler.8(vals.mix$vals.mix, 3))

plot(gibbs.output.8$mu1,ylim=c(-10,15),type="l",main="Gibbs Algorithm Mu Estimates")
lines(gibbs.output.8$mu2,col=2)
lines(gibbs.output.8$mu3,col=3)

#summary table
gibbsgausmix.df <- data.frame(t(apply(gibbs.output.8, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975),na.rm = T))))
colnames(gibbsgausmix.df) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(gibbsgausmix.df, "latex", caption = "Gibbs Algorithm Posterior Distribution", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))


data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu1[500:1000]), sqrt(mean(gibbs.output.8$sigma1[500:1000])), 
                           lam = mean(gibbs.output.8$p1[500:1000])),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu2[500:1000]), sqrt(mean(gibbs.output.8$sigma2[500:1000])), 
                            lam = mean(gibbs.output.8$p2[500:1000])),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu3[500:1000]), sqrt(mean(gibbs.output.8$sigma3[500:1000])), 
                            lam = mean(gibbs.output.8$p3[500:1000])),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("Gibbs Algorithm Posterior Fit")
```

#### Part C
As I mentioned above, both approaches produce very similar results. Unfortunately, neither appear to be able to find the true grouping of normals. I wonder if something is going wrong in my code, if using the k-means labels was potentially an inappropriate way to start my models, my attempts were no better than the one line of code I used to build the k-means labels, or if this is the point of the exercise. If I had more time, I would be interested in testing this out further. However, I am encouraged by getting both models working and that they both create a set of normals that fit our set of 120 samples. 


\newpage
## 9. Chapter 15 Question 3
Regression with many explanatory variables: Table 15.2 displays data from a designed experiment for a chemical process. In using these data to illustrate various approaches to selection and estimation of regression coefficients, Marquardt and Snee (1975) assume a quadratic regression form; that is, a linear relation between the expectation of the untransformed outcome, y, and the variables x1,x2,x3, their two-way interactions, x1x2, x1x3, x2x3, and their squares, x21, x2, x23.

#### Part A

First, I made sure to standardize my predictors along with my outcome (through both mean centering and scaling) as Marquardt and Snee (1975) recommend that we be sure to do. This helps us deal with the collinearity in the predictors that is quite intuitive given that the last six are derived directly from the first three. Now, we run a basic ordinary linear regression model (nonhierarchical with a uniform prior distribution on the coefficients). I used both a frequentist method and a bayesian method with the defined priors to compare the results. I fit the frequentist model using a simple 'lm' function and used the packages 'stan' and 'rstanarm' to build the bayesian. The two were consistent, as I would expect them to be. See the beta coefficient estimates for these models below. I will be reporting the standardized coefficients for ease.   

```{r, 153PartA Prep}
#######################################
# 9. Chapter 15 question 3
#load data
dataset153 <- data.frame(c(1300, 1300, 1300, 1300, 1300, 1300, 1200, 1200, 1200, 1200, 1200, 1200, 1100, 1100, 1100, 1100),
                         c(7.5, 9.0, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 17.0),
                         c(0.0120, 0.0120, 0.0115, 0.0130, 0.0135, 0.0120, 0.0400, 0.0380, 0.0320, 0.0260, 0.0340, 0.0410, 
                           0.0840, 0.0980, 0.0920, 0.0860),
                         c(49.0, 50.2, 50.5, 48.5, 47.5, 44.5, 28.0, 31.5, 34.5, 35.0, 38.0, 38.5, 15.0, 17.0, 20.5, 19.5))
colnames(dataset153) <- c("x1", "x2", "x3", "y")

# standardizations
#get means for later in case
dataset153<-data.frame(scale(dataset153))
```

```{r, MODEL 1 : frequentist model}
# MODEL 1 : frequentist model
freq.mod <- lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), data = dataset153)
#summary(freq.mod)
output <- summary(freq.mod)$coef[, 1:2]
out1 <- cbind(output, confint(freq.mod)[c(1:10),])
colnames(out1) <- c("50%", "sd", "2.5%", "97.5%")
out1 <- out1[, c(3, 1, 4, 2)]
out1 <- data.frame(round(out1, 2))
colnames(out1) <- c("2.5%", "50%", "97.5%", "Std Dev")

kable(out1, "latex", caption = "Frequentist Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
#vif(freq.mod)

```

```{r, MODEL 2 : MCMC regress, eval=F}
# MODEL 2 : use simple MCMC regress (uniform priors?)
library(MCMCpack)
part.a.mod2 <- MCMCregress(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                           data = dataset153, burnin = 1000, mcmc = 10000)
summary(part.a.mod2)
plot(part.a.mod2)
```

```{r, MODEL 3 : use rjags, eval=F}
# MODEL 3 : use rjags
library(rjags)
library(R2jags)

dataset153.dat<-list(y=dataset153$y,
                     x1=dataset153$x1, 
                     x2=dataset153$x2, 
                     x3=dataset153$x3)

cat(
    "model{
    for (i in 1:16) {
      y[i] ~dnorm(mu[i] , tau)
      mu[i] <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    beta0 ~dunif(-1000, 1000)
    beta1  ~dnorm(0, 0.00001)
    beta2 ~dnorm(0, 0.00001)
    beta3 ~dnorm(0, 0.00001)
    tau ~ dinvchi (0.001, 0.001)
    sigma2 <- 1/tau}",
    file="m1.jag"
    )

m1.inits<-list(list("beta0"=1,"beta1"=0,"beta2"=0,"beta3"= 0,
          "tau"=1))

parameters <- c("beta0", "beta1", "beta2", "beta3", "mu", 
           "sigma2")

m1 <- jags(data = dataset153.dat,
        inits = m1.inits,
        parameters.to.save = parameters,
        model.file = "m1.jag",
        n.chains = 1,
        n.iter = 5000,
        n.burnin = 2000,
        n.thin = 1)

m1
plot(m1)        
traceplot(m1)       
plot(as.mcmc(m1))
```

```{r, MODEL 4 : use stan, results='hide',fig.keep='all'}
# MODEL 4 : use stan
library(rstan)
library(rstanarm)
library(bayesplot)

#intercept:constant term,fixed effect
#mean center and scale:standardize

glm_post1 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family=gaussian, 
                      prior=NULL)
```

```{r, MODEL 4 OUTPUT, results='asis',fig.keep='all'}
#summary(glm_post1)
out2 <- summary(glm_post1)[1:10,3:6]
out2 <- out2[, c(2, 3, 4, 1)]
out2 <- data.frame(round(out2, 2))
colnames(out2) <- c("10%", "50%", "90%", "Std Dev")
  
kable(out2, "latex", caption = "Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
#stan_trace(glm_post1)

#posterior predictive chekcs
#pp_check(glm_post1)
#ppc_intervals(y = dataset153$y, yrep = posterior_predict(glm_post1), x = dataset153$x1)

#stan_hist(glm_post1, pars=c("x1"), bins=40)
post_samps_speed <- as.data.frame(glm_post1, pars=c("x1"))[,"x1"]
mn_speed <- mean(post_samps_speed) # posterior mean 
ci_speed <- quantile(post_samps_speed, probs=c(0.05, 0.95)) # posterior 90% interval 

#stan_hist(glm_post1, pars=c("x2"), bins=40)
#stan_hist(glm_post1, pars=c("x3"), bins=40)
```

\newpage
#### Part B
Next, I fit a mixed-effects linear regression model again using the 'stan' and 'rstanarm' packages. As requested, I set a uniform prior distribution on the constant term and a shared normal prior distribution on the coefficients of the nine predictors. If you use iterative simulation in your computations, be sure to use multiple sequences and monitor their joint convergence. See the table below for the predicted coefficient estimates. Below the table is the set of traces plots I used to monitor their joint convergence. Indeed, we see appropriate convergence across multiple sequences.

```{r, 153PartB, results='hide',fig.keep='all'}
glm_post_mixed <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family = gaussian(), 
                      prior = normal(),
                      prior_intercept = NULL)

```

```{r, 153PartB Output, results='asis',fig.keep='all', fig.width=8}
#summary(glm_post_mixed)
out3 <- summary(glm_post_mixed)[1:10,3:6]
out3 <- out3[, c(2, 3, 4, 1)]
out3 <- data.frame(round(out3, 2))
colnames(out3) <- c("10%", "50%", "90%", "Std Dev")

kable(out3, "latex", caption = "Hierarchical Normal Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#tau:chisquarred15.4

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
stan_trace(glm_post_mixed)
```

#### Part C
The model built in Part B appears to be more stable than that of Part A. This assertion is supported by the narrower credible intervals and the smaller standard deviations for $\beta_1$, $\beta_3$, $\beta_4$, and $\beta_8$ in Part B than in Part A. This is a reflection of the shared normal priors drawing the posterior means closer together, rather than allowing for unncessarily extreme values with the non-informative priors. I agree with Marquardt and Snee that model (b) is much preferred in this context. Lastly, by not unscaling the priors I recognize that the magnitude of the changes/differences between the coefficients in Part A and Part B is slightly obscured. However, I opted to leave them standardized to save time, and the expected effect is still displayed. If anything, I would only expect for the unstandardizing process to show the difference are even greater than displayed here.

\newpage
#### Part D
As directed, I fit a mixed-effects linear regression model with a uniform prior distribution on the constant term and a shared t4 prior distribution on the nine predictors. The output is displayed below. The t4 distribution seems to pull the coefficients slightly closer to 0 (the prior mean) than the normal distributions. This is not exactly what I would expected, given the wider tails associated with the student-t distribution. This could potentially have to do with the choice of degrees of freedom, or the data simply might not support the coefficients taking extreme values, even when given a somewhat of an opportunity. They did take extreme values in Part A, however, so maybe this just wasn't *enough* of an opporunity.

```{r, 153PartD, results='hide',fig.keep='all'}
glm_t4 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family = gaussian(), 
                      prior=student_t(df = 4),
                      prior_intercept = NULL)
```

```{r, 153PartD Output, results='asis',fig.keep='all',fig.width=8}
#stan_trace(glm_t4)
#summary(glm_t4)
out4 <- summary(glm_t4)[1:10,3:6]
out4 <- out4[, c(2, 3, 4, 1)]
out4 <- data.frame(round(out4, 2))
colnames(out4) <- c("10%", "50%", "90%", "Std Dev")

kable(out4, "latex", caption = "Hierarchical T4 Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#summary(glm_t4)
stan_trace(glm_t4)
```

#### Part E
Other potential models for the regression coefficients could include Laplace distributions, creating a Lasso regression. This would be and even more severe restriction on the beta values, given that, as a prior, the double exponential function puts even more weight closer to zero. If we wanted to futher parse out which of the coefficients are most supported by the data, that could be a plausible option. On the other hand, if we wanted to allow more flexibility in the coefficients, we could use a t-distribution with a lower degree of freedom (to widen the tails).


\newpage
# Code Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```