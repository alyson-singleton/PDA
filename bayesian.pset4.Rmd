---
title: "PHP2530 Problem Set 4"
author: "Alyson Singleton"
date: "4/21/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, results='asis', warning=F, message=F, cache=T,
                      fig.height=4, fig.width=5, fig.align="center")
pacman::p_load(actuar, dplyr, kableExtra, knitr, LearnBayes, boot, lattice, gtools, ggplot2,
               LaplacesDemon, rstan, rstanarm, bayesplot)
```

## 1. The Importance Sampling Algorithm
#### Part A
```{r, imp.sampA}
# Arguments:

# logTargetDensityFunc is a function of one argument:
# logTargetDensityFunc(xVal)

# logProposalDensityFunc is a function of one arguments:
# logProposalDensityFunc(xVal)

# proposalNewFunc is a function of zero arguments:
# proposalNewFunc( )

# Return Value:

# This function should return nSamples many samples, the
# corresponding log weights, as well as the estimated ESS.

# When the rejectionControlConstant is non-null and a positive
# quantity then perform rejection control and return only the
# accepted samples with their modi_ed log weights, the
# acceptance rate of the samples and the estimated ESS.

ImpSampler <- function(nSamples, logTargetDensityFunc, logProposalDensityFunc, proposalNewFunc, rejectionControlConstant = NULL) {
	#first check what's up with the rejectionControlConstant, i.e. null?
  if (length(rejectionControlConstant)==0) {
    #initialize vectors
    allSamples <- rep(NA,nSamples)
    logWeights <- rep(NA,nSamples) 
    for (i in 1:nSamples){
      allSamples[i] <- proposalNewFunc()
      }
    logWeights <- sapply(allSamples, logTargetDensityFunc) - sapply(allSamples, logProposalDensityFunc)
    finalLogWeights <- logWeights 
    finalSamples <- allSamples
    acceptanceRate <- NA
    effSS <- length(finalLogWeights)/(1 + var(exp(finalLogWeights)))
  }else{ #i.e. rejectionControlConstant was non-null then
    triedLogRs = c()
    finalLogWeights = c()
    finalSamples = c()
    while (length(finalSamples) < nSamples) {
      allSamples <- rep(NA,nSamples)
      logWeights <- rep(NA,nSamples) 
      for (i in 1:nSamples){
        allSamples[i] <- proposalNewFunc()
        }
      logWeights <- sapply(allSamples, logTargetDensityFunc) - sapply(allSamples, logProposalDensityFunc)
      logRs = logWeights - log(rejectionControlConstant)
      logRs[logRs>0] = 0       # = log( min(1,w/c) )
      PassIndicator =  (log(runif(nSamples))< logRs)
      finalSamples = c(finalSamples, allSamples[PassIndicator])
      triedLogRs = c(triedLogRs, logRs)
      finalLogWeights = c(finalLogWeights, logWeights[PassIndicator]-logRs[PassIndicator])
    }
    qc =  mean(exp(triedLogRs))
    acceptanceRate = length(finalSamples)/length(triedLogRs)
    finalLogWeights = finalLogWeights[1:nSamples] + log(qc)
    finalSamples = finalSamples[1:nSamples]
    effSS = nSamples/(1 + var(exp(finalLogWeights))) 
  }
  return(list(finalSamples, finalLogWeights, effSS, acceptanceRate))
}
```
#### Part B/C
```{r, imp.sampBC}
theta <- seq(-6,6,.01)
fx.normal.samples <- (1/3)*dnorm(theta,mean=-2,sd=1) + (2/3)*dnorm(theta,mean=2,sd=1) 
gx.normal.samples <- dnorm(theta,mean=0,sd=3)

plot(theta, fx.normal.samples, ylim=c(0,1.1*max(fx.normal.samples)),
      type="l", xlab="theta", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2)
M <- 1
lines(theta, M*gx.normal.samples)
```

#### Part D

First explore the expectation of $\mu_1$.
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)\right)+E\left(\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = \frac{1}{3}(-2)+\frac{2}{3}(2) = \frac{2}{3}$$

Next investigate the expectation of $\mu_2$.
$$\mu_2 = E\left(\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)^2\right)$$

Finally let us compute the expectation of $\theta$.
$$\theta = E\left(\exp\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)\right)$$
```{r, imp.sampD}
#store the theoretically calculated values in R
mu1True <- 2/3 
mu2True <- 5 
thetaTrue <- (exp(-3/2)/3) + (2*exp(5/2)/3)
```

#### Part E
```{r, imp.sampE}
## write f and g as r functions
fx <- function(x){
  (1/3)*dnorm(x,mean=-2,sd=1) + (2/3)*dnorm(x,mean=2,sd=1) 
} 
gx <- function(x){
  dnorm(x,mean=0,sd=3)
} 

## write them taking the log
logfx <- function(x){
  log((1/3)*dnorm(x,mean=-2,sd=1) + (2/3)*dnorm(x,mean=2,sd=1)) 
} 
loggx <- function(x){
  log(dnorm(x,mean=0,sd=3))
} 
gSampleFunc <- function(){ rnorm( 1 , mean = 0 , sd = 3 ) }
# Function to calculate the mean of the function h(x):
muhat <- function(hFunc, ISObject){ 
  return(sum(hFunc(ISObject[[1]]) * exp(ISObject[[2]]))/sum(exp(ISObject[[2]])))
  }
####importance sampling without rejection control
mixNormSampler <- ImpSampler(5000, logfx, loggx, gSampleFunc)
mu1 <- function(x){x}
mu2 <- function(x){x^2}
theta <- function(x){exp(x)}
estimates = c(muhat(mu1, mixNormSampler), muhat(mu2, mixNormSampler), muhat(theta, mixNormSampler))
estimates
# 0.7225219 4.9041803 8.0581787
abs(estimates - c(mu1True,mu2True,thetaTrue))   #error
# [1] 0.05585519 0.09581972 0.13786070
mixNormSampler[[3]] # The ESS:
# [1] 3185.469
```

#### Part F
```{r, imp.sampF}
####importance sampling with rejection control (1:10)
#initialize
accRates <- NULL ; effSSize <- NULL ; mu1Errors <- NULL ; mu2Errors <- NULL ; thetaErrors <- NULL
#run through and store output with varying values for the rejection control as directed
for (i in 1:10) {
  mixNormSamp <- ImpSampler(5000, logfx, loggx, gSampleFunc, rejectionControlConstant = i)
  accRates <- c(accRates, mixNormSamp[[4]])
  effSSize <- c(effSSize, mixNormSamp[[3]])
  mu1Errors <- c(mu1Errors, abs(muhat(mu1, mixNormSamp) - mu1True))
  mu2Errors <- c(mu2Errors, abs(muhat(mu2, mixNormSamp) - mu2True))
  thetaErrors <- c(thetaErrors, abs(muhat(theta, mixNormSamp) - thetaTrue))
}
round(rbind(mu1Errors, mu2Errors, thetaErrors, accRates, effSSize), 2)
#             [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9] [,10]
#mu1Errors    0.02    0.04    0.06    0.04    0.02    0.03    0.02    0.01    0.04 2e-02
#mu2Errors    0.02    0.09    0.14    0.09    0.04    0.06    0.04    0.05    0.03 2e-02
#mu3Errors    0.02    0.31    0.24    0.20    0.17    0.17    0.20    0.10    0.06 5e-02
#accRates     0.70    0.47    0.34    0.25    0.20    0.17    0.14    0.12    0.11 1e-01
#effSSize  4347.45 4956.41 5000.00 5000.00 5000.00 5000.00 5000.00 5000.00 5000.00 5e+03


par(mfrow = c(1, 3))
plot(mu1Errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu1", ylab = "Absolute error")
plot(mu2Errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu2", ylab = "Absolute error")
plot(thetaErrors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu3", ylab = "Absolute error" )


par(mfrow = c(1, 2))
plot(accRates, ylim = c(0, 1), type = "l", xlab = "Rejection control constant", main = "Plot of acceptance rates", ylab = "Acceptance rate")
plot(effSSize, ylim = c(0, 5000), type = "l", xlab = "Rejection control constant", main = "Plot of ess", ylab = "ess")
#accRates decreasing with C
#effSSize increading with C
```


## 2. Chapter 10 Question 5
#### Part A
Indeed, we are given the structure of the model from the problem statement. The information we have is as follows, where $J=10$:
$$p(y_j|\theta_j) \sim \text{Bin}(n_j,\theta_j), \text{ i.e.   } p(y|\theta,\alpha,\beta) = \prod_{j=1}^{10}{n_j \choose y_i}\theta_j^{y_i}(1-\theta_j)^{n_j-y_i}$$
$$p(\theta_j) \sim \text{logit}^{-1}(\alpha+\beta x_j)$$
$$\alpha ∼ t_4(0, 2^2) \text{ and } \beta ∼ t_4(0, 1^2)$$
$$x_j ∼ U(0, 1) \text{ and } n_j ∼ \text{Poiss}(5)$$
I build the dataset of 10 samples from this model as directed (Table 1).
```{r, 10.5A}
#build random dataset from the model
yis <- c(rep(NA, 10))
nis <- c(rep(NA, 10))
xis <- c(rep(NA, 10))
alphais <- c(rep(NA, 10))
betais <- c(rep(NA, 10))

set.seed(1)
for (i in 1:10) {
  alpha <- rmt(1,0,2,4)
  alphais[i] <- alpha
  
  beta <- rmt(1,0,1,4)
  betais[i] <- beta
  
  xi <- runif(1, min=0, max=1)
  xis[i] <- xi
  
  ni <- rpois(1,5)
  nis[i] <- ni
  
  p <- inv.logit(alpha + beta*xi)
  
  yis[i] <- rbinom(1, ni, p)
}

dataset <- cbind(yis,nis)
dataset.df <- t(data.frame(dataset))
kable(dataset.df, "latex", caption = "10.5 Sampled Data", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part B
Next, I used rejection sampling to acquire to get 1000 independent posterior draws from $(\alpha, \beta)$. The likelihood follows:
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta)\prod_{i=1}^k p(y_i|\alpha,\beta,n_i,x_i)$$
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta) \prod_{i=1}^k [\text{logit}^{-1}(\alpha+\beta x_i)]^{y_i}[1-\text{logit}^{-1}(\alpha+\beta x_i)]^{n_i-y_i}$$
Notably, 

First, I display the contours of the joint posterior distribution of $(\alpha, \beta)$ for your reference. The following plot shows the draws from the rejection sampling.

```{r, 10.5B}
library(LearnBayes)
#posterior function
logitBin = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  N = length(xis)
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*xis)) + 
    (n-y)*log((1-inv.logit(alpha + beta*xis)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  post = loglikelihood #+ logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 30
x0 = seq(-5, 5, len = ng)
y0 = seq(-15, 10, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin, data = dataset)
Z = Z - max(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Contour plot of joint posterior")

####Rejection sampling
#find the mode
tpar=LearnBayes::laplace(logitBin,array(c(3,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar)
{
	data=data
	d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,
	log=TRUE)
	return(d)
}

start=array(c(3,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT,start,data=dataset, tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

rejectsamp=function(logf,tpar,dmax,n,data){
	theta=rmt(n,mean=c(tpar$mode),S=2*tpar$var,df=4)
	lf=apply(theta,1,logf,data=data)
	lg=dmt(theta,mean=c(tpar$mode),S=2*tpar$var,df=4,log=TRUE)
	prob=exp(lf-lg-dmax)
	return(theta[runif(n)<prob,])
}

theta=rejectsamp(logitBin,tpar,logitBinT(fit1$mode,dataset,tpar),2000,dataset)
#dim(theta)
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Rejection Sampling",
        drawlabels = F)
points(theta[,1],theta[,2])
```

#### Part C

Here we use the Laplace function to estimate the approximate the posterior density for $(\alpha,\beta)$ with a normal centered at the posterior mode with covariance matrix fit to the curvature at the mode. See below for the estimates:

```{r, 10.5C}
# build normal approximation
tpar=LearnBayes::laplace(logitBin,array(c(-1,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar) {
  data=data
  d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
  return(d)
}
start=array(c(-1,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT,start,data=dataset,tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

dataset.df10.5C1 <- data.frame(fit1$mode)
colnames(dataset.df10.5C1) <- c("alpha", "beta")
rownames(dataset.df10.5C1) <- c("")
kable(dataset.df10.5C1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

dataset.df10.5C2 <- data.frame(fit1$var)
colnames(dataset.df10.5C2) <- c("alpha", "beta")
rownames(dataset.df10.5C2) <- c("alpha", "beta")
kable(dataset.df10.5C2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part D
Using the calculated posterior mode and covariance matrix to build two-dimensional $t_4$ distributions, I employed importance sampling to estimate $E(alpha|y)$ and $E(\beta|y)$. First, see the samples displayed on the contour plot. The table below that displayes the distribution of the estimates for $(\alpha,\beta)$. The means of the 1,000 samples were calculated as 0.205 for $\alpha$ and -2.01 for $\beta$.

```{r, 10.5D}
##importance sampling using normal centered at the posterior mode with covariance 
    #matrix fit to the curvature at the mode
theta = rmt(1000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin,data=dataset)
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
wt = exp(lf - lp)
probs=wt/sum(wt, na.rm = T)
probs[is.na(probs)] <- 0
theta.S=theta[sample(1:1000,size=1000,prob=probs,replace=TRUE),]

contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2, 
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Importance Sampling",
        drawlabels = F)
points(theta.S[,1],theta.S[,2])

#estimates
dataset.df10.5D <- t(data.frame(apply(theta.S,2,summary)))
rownames(dataset.df10.5D) <- c("alpha", "beta")
kable(dataset.df10.5D, "latex", caption = "10.5 Estimates for Expectations", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part E
I calculated the effective sample size to be 1.63. Given it's small size this would lead me to believe that there are likely a few extremely high weights which might be unduly influencing the distribution.

```{r, 10.5E}
normalize = function(x){return(x/sum(x))}
seff <- 1/sum(normalize(wt)^2)
#seff <- length(wt)/(1 + var(wt))
#seff
```


## 3. Chapter 10 Question 8
#### Part A

First, I set out to recreate the approximate posterior distribution of the bioassay example in Section 3.7. My first plot below is my recreation of Figure 3.3b to confirm that I am recreating the distribution correctly. After sampling 10,000 from the approximate distribution, I resampled without replacement $k = 1000$ samples. The plot of those draws are shown on the next plot.

```{r, 108A}
# load data
x108 <- c(-0.86,-0.30,-0.05,0.74) 
n108 <- rep(5,length(x108))
y108 <- c(0,1,3,5)
dataset108<-cbind(y108,n108)

#posterior function
logitBin108 = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*x108)) + 
    (n108-y)*log((1-inv.logit(alpha + beta*x108)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  logPrior <- 0 + 0 #non informative 
  post = loglikelihood + logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 50
#grid suggestion from book
x0 = seq(-2, 7, len = ng)
y0 = seq(-2, 35, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin108, data = dataset108)
Z = exp(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Contour Plots to Compare to Figure 3.3b",
        drawlabels = F)

# build normal approximation
tpar=LearnBayes::laplace(logitBin108,array(c(1,10),c(1,2)),data=dataset108)
betabinT108=function(theta,data,tpar)
{
	data=data
	d=logitBin108(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
	return(d)
}
start=array(c(1,10),c(1,2))
fit1=LearnBayes::laplace(betabinT108,start,data=dataset108, tpar=tpar)
#betabinT108(fit1$mode,dataset108,tpar)

#importance resampling SIR without replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=F),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling Without Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```

#### Part B
See below for my first attempt at displaying the distribution of the simulated importance ratios. 

```{r}
hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios with Outliers")
```

For a more representative display of the simulated importance ratios, I remove multiple outliers from the upper end of the distribution. The resulting distribution is displayed below. The importance ratios appear to ratios vary as we would hope! 

```{r}
#remove outliers
for(i in 1:40){
  wt<-wt[wt < max(wt)]
}

hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios without Outliers")
```

#### Part C

Below I display the results from doing importance resampling with replacement. There does not appear to be much difference between the samples drawn when replacing that in Part A where there was none. This is an indicator that the importance weights are moderate, whichi is in line with what we saw in Part B. In situations like these, it would make sense that sampling with and without replacement gives similar results (as compared to a situation where there are a few large weights and many small weights and sampling with replacement would pick the same few values repeatedly, leading to a different set that a more conservative without replacement approach).

```{r, 108C}
#importance resampling SIR WITH replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=T),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling With Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```




## 4. Chapter 11 Question 2
```{r, 112}
#initializations
sims <- 10000
theta <-  matrix(0, nrow = sims, ncol = 2) 
r.all <- rep(0,sims) 

#starting positions found from normal approximation
theta[1,] <- c(0.847,7.749)

#jumping function
jumping <- function(theta, scale=.1) rmnorm(1, mean = c(tpar$mode), varcov = matrix(tpar$var,2,2))

#metropolis algorithm
for(i in 2:sims) {
  theta.new = jumping(theta[i-1,], .1)
  uu = runif(1)
  r <- min(exp(logitBin108(theta.new, dataset108) - logitBin108(theta[i-1,], dataset108)), 1)
  if (uu < r) {
    theta[i,] <- theta.new
  }else{
    theta[i,] = theta[i-1,]
  }
}

# plots for convergence? / investigation / display
firstChain <- c(rep(NA, sims))
for (i in 1:sims) {
  firstChain[i] <- exp(logitBin108(theta[i,],dataset108))
}
# probability
plot(seq(1:sims),firstChain,type="l",xlab="Iteration",ylab="posterior prob",main="post convergence?")
#alpha
plot(seq(1:sims),theta[,1],type="l",xlab="Iteration",ylab="posterior prob",main="alpha convergence?")
#beta
plot(seq(1:sims),theta[,2],type="l",xlab="Iteration",ylab="posterior prob",main="beta convergence?")

#samples/walks
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha")
points(theta[,1],theta[,2], pch='.')
```

## 5. Chapter 11 Question 3
```{r, 113}
#load in data
dataset113 <-  data.frame(machineno = rep(1:6, each=5), 
                          ments = c(83, 92, 92, 46, 67,
                                    117, 109, 114, 104, 87,
                                    101, 93, 92, 86, 67,
                                    105, 119, 116, 102, 116,
                                    79, 97, 103, 79, 92,
                                    57, 92, 104, 77, 100))
J <- length(unique(dataset113$machineno))
n <- length(dataset113$ments)

#starting points
# from text "obtain 10 starting points for the simulations by drawing thetaj independently in this way for each group"

set.seed(142857)
theta.start <- sapply(1:6,function(x) sample(dataset113$ments[dataset113$machineno==x], 10, replace=TRUE))
mu.start <- apply(theta.start, 2, mean)
sigma.start <- sqrt(apply(theta.start, 2, var))

# ____________________________________________________________________________________________________________________

#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}
#sigma (step 3)
sigma.post.sample <-  function(theta) {
  #first find sigma hat
  sigma.hat.func <- function(theta) {
    sigma.hat <- sapply(1:6, function(x) (dataset113$ments[dataset113$machineno==x] - theta[x])^2)  
    sigma.hat <- (1/n) * sum(unlist(sigma.hat))
    return(sigma.hat)
  }
  #next find conditional sigma
  sigma.hat <- sigma.hat.func(theta) 
  sigma.cond <- rinvchisq(1,n,sigma.hat)
  return(sigma.cond)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma)+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6", 
                               "mu", "sigma2", "tau2")
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,9] <- tau.post.sample(sample.i)
  param.storage[1,8] <- sigma.post.sample(sample.i)
  param.storage[1,7] <- mu.post.sample(sample.i,param.storage[1,9])#param.storage[1,9])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,7], param.storage[i-1,8], param.storage[i-1,9])
    param.storage[i,9] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,8] <- sigma.post.sample(param.storage[i,1:6])
    param.storage[i,7] <- mu.post.sample(param.storage[i,1:6], param.storage[i,9])
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,8:9] <- sqrt(gibbs.output[,8:9])

#summary table
t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975))))

# ____________________________________________________________________________________________________________________

#measures for theta 6 & grand.mean for seperate model
y.bar.6.dot <- mean(dataset113$ments[dataset113$machineno==6]) 
var.sep.6 <- var(dataset113$ments[dataset113$machineno==6])
theta.sep.6 <- rnorm(sims, y.bar.6.dot, sqrt(var.sep.6))

theta.sep.7 <- mean(dataset113$ments)

#measures for theta 6 & grand.mean for pooled model
y.bar.dot.dot <- mean(dataset113$ments)
var.pooled <- sum((dataset113$ments[dataset113$machineno==6] - mean(dataset113$ments))^2) / 
  (length(dataset113$ments) - 1)
theta.pooled.6 <- rnorm(sims, y.bar.dot.dot, sqrt(var.pooled))


# ____________________________________________________________________________________________________________________


# posterior distribution of the mean of the quality measurements of the sixth machine
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))

# predictive distribution for another quality measurement of the sixth machine (maximum?)
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
lines(x=c(y.bar.6.dot,y.bar.6.dot),y=c(-1,1), col="black")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))

# posterior distribution of the mean of the quality measurements of the potential seventh machine
plot(density(gibbs.output[,"mu"]), col="red", xlab="Mean Measure", 
    ylab="Density", main="Mean of Potential Machine 7")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
legend("topright",col = c("red","blue", "green"), 
       legend=c("hierarchical","pooled", "separated"), 
       lty = c(1,1,1)) 
```


## 6. Chapter 11 Question 4
```{r, 114}
#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}

# change sigma function
sigma.post.sample.p2 <- function(theta, mu) {
  sigma2.star <- NULL
  for(j in 1:J)	{
    k <- 5#length( y )
    nu.k <- 1 + k
    sigma.0.grid <- seq(0.25, 30, 0.25)
    sigma2.star.vals<-NULL
    probs <- NULL
    for (i in 1:length(sigma.0.grid)) {
      v <- mean((theta[j] - mu)^2)
      sigma2.k.option <- (1 * sigma.0.grid[i] + k*v) / (1 + k) #nu.0=1
      sigma2.star.vals[i] <- rinvchisq(1, nu.k, sigma2.k.option)
      probs[i] <- dinvchisq(sigma2.star.vals, nu.k, sigma2.k.option)
    }
    #print(mean(probs))
    sigma2.star[j] <- sample(sigma2.star.vals, 1, prob=probs)#rinvchisq(1, nu.k, sigma2.k)  
  }
  return(sigma2.star)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma[j])+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma[j]) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 14
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6",
                               "sigma1", "sigma2", "sigma3", "sigma4", "sigma5", "sigma6",
                               "tau2","mu")
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,13] <- tau.post.sample(sample.i)
  param.storage[1,14] <- mu.post.sample(sample.i,param.storage[1,13])
  param.storage[1,7:12] <- sigma.post.sample.p2(sample.i,param.storage[1,14])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,14], param.storage[i-1,7:12], param.storage[i-1,13])
    param.storage[i,13] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,14] <- mu.post.sample(param.storage[i,1:6], param.storage[i,13])
    param.storage[i,7:12] <- sigma.post.sample.p2(param.storage[i,1:6], param.storage[i,14])
    
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,7:13] <- sqrt(gibbs.output[,7:13])

#summary table
t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975))))
```





## 7. Chapter 13 Question 5
Estimating the number of unseen species (see Fisher, Corbet, and Williams, 1943, Efron and Thisted, 1976, and Seber, 1992): suppose that during an animal trapping expedition the number of times an animal from species i is caught is $x_i ~ Poisson(\lambda_i)$. For parts (a)–(d) of this problem, assume a $Gamma(\alpha,\beta)$ prior distribution for the $\lambda_i$’s, with a uniform hyperprior distribution on $(\alpha,\beta)$. The only observed data are $y_k$, the number of species observed exactly k times during a trapping expedition, for $k = 1, 2, 3,...$

#### Part A
Write the distribution $p(x_i|\alpha,\beta)$.

#### Part B
Use the distribution of $x_i$ to derive a multinomial distribution for y given that there are a total of N species.

#### Part C
Suppose that we are given y = (118, 74, 44, 24, 29, 22, 20, 14, 20, 15, 12, 14, 6, 12, 6, 9, 9, 6, 10, 10, 11, 5, 3, 3), so that 118 species were observed only once, 74 species were observed twice, and so forth, with a total of 496 species observed and 3266 animals caught. Write down the likelihood for y using the multinomial distribution with 24 cells (ignoring unseen species). Use any method to find the mode of $(\alpha,\beta)$ and an approximate second derivative matrix.

#### Part D
Derive an estimate and approximate 95% posterior interval for the number of additional species that would be observed if 10,000 more animals were caught.

#### Part E
Evaluate the fit of the model to the data using appropriate posterior predictive checks.

#### Part F
Discuss the sensitivity of the inference in (d) to each of the model assumptions.








## 8. Generate / simulate n = 120 observations from a three component mixture 

```{r, Prep}
set.seed(100)
#investigate model and pull 120 observations as directed
y <- seq(-50,50,0.1)
#dens <- function (x, theta, stnd){dnorm (x, theta, sqrt(1000*theta*(1-theta)))}
dens.mix <- 0.1*dnorm(y,0,1) + 0.3*dnorm(y,-2,2) + 0.6*dnorm(y,3,16)
plot (y, dens.mix, ylim=c(0,1.1*max(dens.mix)),
      type="l", xlab="y", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2)

vals.mix <- c(rnorm(120*0.1,0,1), rnorm(120*0.3,-2,2), rnorm(0.6*120,3,16))
labels <- c(rep(1, 12), rep(2, 36), rep(3, 72))
vals.mix <- data.frame(cbind(vals.mix, labels))

#use k means to initialize
vals.kmeans <- kmeans(vals.mix$vals.mix, 3) 
vals.kmeans.cluster <- vals.kmeans$cluster
vals.df <- data.frame(x = vals.mix$vals.mix, 
                      cluster = vals.kmeans.cluster)

vals.df %>%
  mutate(num = row_number()) %>%
  ggplot(aes(y = num, x = x, color = factor(cluster))) +
  geom_point() +
  ylab("Values") +
  ylab("Data Point Number") +
  scale_color_discrete(name = "Cluster") +
  ggtitle("K-means Clustering")

vals.summary.df <- vals.df %>%
  group_by(cluster) %>%
  summarize(mu = mean(x), variance = var(x), std = sd(x), size = n()) %>%
  mutate(p = size / sum(size))
```

#### Part A
```{r, 8A}
#e step
e_step <- function(x, mu.vector, sd.vector, p.vector) {
  comp1.prod <- dnorm(x, mu.vector[1], sd.vector[1]) * p.vector[1]
  comp2.prod <- dnorm(x, mu.vector[2], sd.vector[2]) * p.vector[2]
  comp3.prod <- dnorm(x, mu.vector[3], sd.vector[3]) * p.vector[3]
  sum.of.comps <- comp1.prod + comp2.prod + comp3.prod
  comp1.post <- comp1.prod / sum.of.comps
  comp2.post <- comp2.prod / sum.of.comps
  comp3.post <- comp3.prod / sum.of.comps

  sum.of.comps.log.sum <- sum(log(sum.of.comps))

  list("loglik" = sum.of.comps.log.sum,
       "posterior.df" = cbind(comp1.post, comp2.post, comp3.post))
}

#m step
m_step <- function(x, posterior.df) {
  comp1.n <- sum(posterior.df[,1])
  comp2.n <- sum(posterior.df[,2])
  comp3.n <- sum(posterior.df[,3])
  
  comp1.mu <- 1/comp1.n * sum(posterior.df[,1] * x)
  comp2.mu <- 1/comp2.n * sum(posterior.df[,2] * x)
  comp3.mu <- 1/comp3.n * sum(posterior.df[,3] * x)

  comp1.var <- sum(posterior.df[,1] * (x - comp1.mu)^2) * 1/comp1.n
  comp2.var <- sum(posterior.df[,2] * (x - comp2.mu)^2) * 1/comp2.n
  comp3.var <- sum(posterior.df[,3] * (x - comp3.mu)^2) * 1/comp3.n

  comp1.p <- comp1.n / length(x)
  comp2.p <- comp2.n / length(x)
  comp3.p <- comp3.n / length(x)
  
  list("mu" = c(comp1.mu, comp2.mu, comp3.mu),
       "var" = c(comp1.var, comp2.var, comp3.var),
       "p" = c(comp1.p, comp2.p, comp3.p))
}

#loop function
for (i in 1:2000) {
  if (i == 1) {
    # Initialization
    e.step <- e_step(vals.mix$vals.mix, 
                     vals.summary.df[["mu"]], 
                     vals.summary.df[["std"]],
                     vals.summary.df[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    cur.loglik <- e.step[["loglik"]]
    loglik.vector <- e.step[["loglik"]]
    
  } else {
    # Repeat E and M steps till convergence
    e.step <- e_step(vals.mix$vals.mix, m.step[["mu"]], 
                     sqrt(m.step[["var"]]), 
                     m.step[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    loglik.vector <- c(loglik.vector, 
                       e.step[["loglik"]])
    loglik.diff <- abs((cur.loglik - e.step[["loglik"]]))
    if(loglik.diff < 1e-6) {
      break
    } else {
      cur.loglik <- e.step[["loglik"]]
    }
  }
}

loglik.vector
m.step

plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[1], sqrt(m.step$var[1]), 
                           lam = m.step$p[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[2], sqrt(m.step$var[2]), 
                            lam = m.step$p[2]),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[3], sqrt(m.step$var[3]), 
                            lam = m.step$p[3]),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("Final GMM Fit")
```

#### Part B
```{r, 8B}
# mixed gaussian model functions

normalize = function(x){return(x/sum(x))}

sample_z = function(x,pi,mu){
  dmat = outer(mu,x,"-") # k by n matrix, d_kj =(mu_k - x_j)
  p.z.given.x = as.vector(pi) * dnorm(dmat,0,1) 
  p.z.given.x = apply(p.z.given.x,2,normalize) # normalize columns
  z = rep(0, length(x))
  for(i in 1:length(z)){
    z[i] = sample(1:length(pi), size=1,prob=p.z.given.x[,i],replace=TRUE)
  }
  return(z)
}

sample.pi.post = function(z,k) {
  counts = colSums(outer(z,1:k,FUN="=="))
  pi = rdirichlet(1,counts+1)
  return(pi)
}

sample_mu = function(x, z, k, prior){
  df = data.frame(x=x,z=z)
  mu = rep(0,k)
  for(i in 1:k){
    sample.size = sum(z==i)
    sample.mean = ifelse(sample.size==0,0,mean(x[z==i]))
    
    post.prec = sample.size+prior$prec
    post.mean = (prior$mean * prior$prec + sample.mean * sample.size)/post.prec
    mu[i] = rnorm(1,post.mean,sqrt(1/post.prec))
  }
  return(mu)
}

#gibbs sampler function
sims <- 1000

gibbs.sampler.8 <- function(x, k, muprior = list(mean=0,prec=0.1)) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, nrow=sims, ncol=no.params)
  colnames(param.storage) <- c("mu1", "mu2", "mu3",
                               "sigma1", "sigma2", "sigma3",
                               "p1", "p2", "p3")
  z.storage <- matrix(NA, nrow=sims, ncol=length(vals.mix$vals.mix))
  
  #starting values for theta, tau, sigma and mu (calculated from our sample build earlier, theta.start)
  param.storage[1,1:3] <- vals.summary.df$mu
  param.storage[1,4:6] <- vals.summary.df$variance
  param.storage[1,7:9] <- vals.summary.df$p
  z.storage[1,] <- vals.kmeans.cluster
  
  #interations
  for (i in 2:sims) {
    z.storage[i,] = sample_z(x,param.storage[i-1,7:9],param.storage[i-1,1:3])
    param.storage[i,7:9] = sample.pi.post(z.storage[i,],k)
    param.storage[i,1:3] = sample_mu(x,z.storage[i,],k,muprior)
  }
return(param.storage)
}

gibbs.output.8 <- data.frame(gibbs.sampler.8(vals.mix$vals.mix, 3))
#gibbs.output.8 <- gibbs.output.8[800:1000,]

plot(gibbs.output.8$mu1,ylim=c(-30,30),type="l")
lines(gibbs.output.8$mu2,col=2)
lines(gibbs.output.8$mu3,col=3)

#summary table
t(apply(gibbs.output.8, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975),na.rm = T)))


data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[1], sqrt(m.step$var[1]), 
                           lam = m.step$p[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[2], sqrt(m.step$var[2]), 
                            lam = m.step$p[2]),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[3], sqrt(m.step$var[3]), 
                            lam = m.step$p[3]),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("Final GMM Fit")

```

#### Part C
Compare the two.








## 9. Chapter 15 question 3
Regression with many explanatory variables: Table 15.2 displays data from a designed experiment for a chemical process. In using these data to illustrate various approaches to selection and estimation of regression coefficients, Marquardt and Snee (1975) assume a quadratic regression form; that is, a linear relation between the expectation of the untransformed outcome, y, and the variables x1,x2,x3, their two-way interactions, x1x2, x1x3, x2x3, and their squares, x21, x2, x23.

#### Part A
Fit an ordinary linear regression model (that is, nonhierarchical with a uniform prior distribution on the coefficients), including a constant term and the nine explanatory variables above.

```{r, 153PartA Prep}
#load data
dataset153 <- data.frame(c(1300, 1300, 1300, 1300, 1300, 1300, 1200, 1200, 1200, 1200, 1200, 1200, 1100, 1100, 1100, 1100),
                         c(7.5, 9.0, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 17.0),
                         c(0.0120, 0.0120, 0.0115, 0.0130, 0.0135, 0.0120, 0.0400, 0.0380, 0.0320, 0.0260, 0.0340, 0.0410, 
                           0.0840, 0.0980, 0.0920, 0.0860),
                         c(49.0, 50.2, 50.5, 48.5, 47.5, 44.5, 28.0, 31.5, 34.5, 35.0, 38.0, 38.5, 15.0, 17.0, 20.5, 19.5))
colnames(dataset153) <- c("x1", "x2", "x3", "y")
```

```{r, MODEL 1 : frequentist model}
# MODEL 1 : frequentist model
library(car)
freq.mod <- lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + poly(x1,2) + poly(x2,2) + poly(x3,2), data = dataset153)
summary(freq.mod)
output <- summary(freq.mod)$coef[, 1:2]
out <- cbind(output, confint(freq.mod)[c(1:4,6,8,10:13),])
colnames(out) <- c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)

freq.mod <- lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), data = dataset153)
summary(freq.mod)
output <- summary(freq.mod)$coef[, 1:2]
out <- cbind(output, confint(freq.mod)[c(1:10),])
colnames(out) <- c("posterior mean", "posterior std", "2.5", "97.5")
round(out, 2)
vif(freq.mod)

```

```{r, MODEL 2 : MCMC regress}
# MODEL 2 : use simple MCMC regress (uniform priors?)
library(MCMCpack)
part.a.mod2 <- MCMCregress(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                           data = dataset153, burnin = 1000, mcmc = 10000)
summary(part.a.mod2)
plot(part.a.mod2)
```

```{r, MODEL 3 : use rjags, eval=F}
# MODEL 3 : use rjags
library(rjags)
library(R2jags)

dataset153.dat<-list(y=dataset153$y,
                     x1=dataset153$x1, x2=dataset153$x2, x3=dataset153$x3)

cat(
    "model{
    for (i in 1:16) {
      y[i] ~dnorm(mu[i] , tau)
      mu[i] <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    beta0 ~dnorm(0, 0.00001)
    beta1  ~dnorm(0, 0.00001)
    beta2 ~dnorm(0, 0.00001)
    beta3 ~dnorm(0, 0.00001)
    tau ~ dgamma (0.001, 0.001)
    sigma2 <- 1/tau}",
    file="m1.jag"
    )

m1.inits<-list(list("beta0"=1,"beta1"=0,"beta2"=0,"beta3"= 0,
          "tau"=1))

parameters <- c("beta0", "beta1", "beta2", "beta3", "mu", 
           "sigma2")

m1 <- jags(data = dataset153.dat,
        inits = m1.inits,
        parameters.to.save = parameters,
        model.file = "m1.jag",
        n.chains = 1,
        n.iter = 5000,
        n.burnin = 2000,
        n.thin = 1)

m1
plot(m1)        
traceplot(m1)       
plot(as.mcmc(m1))
```

```{r, MODEL 4 : use stan, eval=F}
# MODEL 4 : use stan
library(rstan)
library(rstanarm)
library(bayesplot)

glm_post1 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family=gaussian, 
                      prior=NULL)
summary(glm_post1)

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
stan_trace(glm_post1)

#posterior predictive chekcs
pp_check(glm_post1)
ppc_intervals(y = dataset153$y, yrep = posterior_predict(glm_post1),x = dataset153$x1)

stan_hist(glm_post1, pars=c("x1"), bins=40)
post_samps_speed <- as.data.frame(glm_post1, pars=c("x1"))[,"x1"]
mn_speed <- mean(post_samps_speed) # posterior mean 
ci_speed <- quantile(post_samps_speed, probs=c(0.05, 0.95)) # posterior 90% interval 

stan_hist(glm_post1, pars=c("x2"), bins=40)
stan_hist(glm_post1, pars=c("x3"), bins=40)
```

#### Part B
Fit a mixed-effects linear regression model with a uniform prior distribution on the constant term and a shared normal prior distribution on the coefficients of the nine variables above. If you use iterative simulation in your computations, be sure to use multiple sequences and monitor their joint convergence.

```{r, 153PartB, eval=F}
glm_post_mixed <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family=gaussian, 
                      prior=normal(0,5),
                      prior_intercept = NULL)
summary(glm_post_mixed)

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
stan_trace(glm_post_mixed)
```

#### Part C
Discuss the differences between the inferences in (a) and (b). Interpret the differences in terms of the hierarchical variance parameter. Do you agree with Marquardt and Snee that the inferences from (a) are unacceptable?

#### Part D
Repeat (a), but with a t4 prior distribution on the nine variables.

```{r, 153PartD, eval=F}
glm_post2 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family=gaussian, 
                      prior=student_t(df = 4))
summary(glm_post_mixed)
stan_trace(glm_post_mixed)
```

#### Part E
Discuss other models for the regression coefficients.






# Code Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```