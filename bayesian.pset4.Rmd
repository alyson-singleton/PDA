---
title: "PHP2530 Problem Set 4"
author: "Alyson Singleton"
date: "4/21/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, results='asis', warning=F, message=F, cache=T,
                      fig.height=4, fig.width=5, fig.align="center")
pacman::p_load(actuar, dplyr, kableExtra, knitr, LearnBayes, boot, lattice, gtools, ggplot2,
               LaplacesDemon, rstan, rstanarm, bayesplot)
```

## 1* The Importance Sampling Algorithm
#### Part A
```{r, imp.sampA}
# Arguments:

# logTargetDensityFunc is a function of one argument:
# logTargetDensityFunc(xVal)

# logProposalDensityFunc is a function of one arguments:
# logProposalDensityFunc(xVal)

# proposalNewFunc is a function of zero arguments:
# proposalNewFunc( )

# Return Value:

# This function should return nSamples many samples, the
# corresponding log weights, as well as the estimated ESS.

# When the rejectionControlConstant is non-null and a positive
# quantity then perform rejection control and return only the
# accepted samples with their modi_ed log weights, the
# acceptance rate of the samples and the estimated ESS.

ImpSampler <- function(nSamples, logTargetDensityFunc, logProposalDensityFunc, proposalNewFunc, rejectionControlConstant = NULL) {
	#first check what's up with the rejectionControlConstant, i.e. null?
  if (length(rejectionControlConstant)==0) {
    #initialize vectors
    allSamples <- rep(NA,nSamples)
    logWeights <- rep(NA,nSamples) 
    for (i in 1:nSamples){
      allSamples[i] <- proposalNewFunc()
      }
    logWeights <- sapply(allSamples, logTargetDensityFunc) - sapply(allSamples, logProposalDensityFunc)
    finalLogWeights <- logWeights 
    finalSamples <- allSamples
    acceptanceRate <- NA
    effSS <- length(finalLogWeights)/(1 + var(exp(finalLogWeights)))
  }else{ #i.e. rejectionControlConstant was non-null then
    triedLogRs = c()
    finalLogWeights = c()
    finalSamples = c()
    while (length(finalSamples) < nSamples) {
      allSamples <- rep(NA,nSamples)
      logWeights <- rep(NA,nSamples) 
      for (i in 1:nSamples){
        allSamples[i] <- proposalNewFunc()
        }
      logWeights <- sapply(allSamples, logTargetDensityFunc) - sapply(allSamples, logProposalDensityFunc)
      logRs = logWeights - log(rejectionControlConstant)
      logRs[logRs>0] = 0       # = log( min(1,w/c) )
      PassIndicator =  (log(runif(nSamples))< logRs)
      finalSamples = c(finalSamples, allSamples[PassIndicator])
      triedLogRs = c(triedLogRs, logRs)
      finalLogWeights = c(finalLogWeights, logWeights[PassIndicator]-logRs[PassIndicator])
    }
    qc =  mean(exp(triedLogRs))
    acceptanceRate = length(finalSamples)/length(triedLogRs)
    finalLogWeights = finalLogWeights[1:nSamples] + log(qc)
    finalSamples = finalSamples[1:nSamples]
    effSS = nSamples/(1 + var(exp(finalLogWeights))) 
  }
  return(list(finalSamples, finalLogWeights, effSS, acceptanceRate))
}
```
#### Part B/C
```{r, imp.sampBC}
theta <- seq(-6,6,.01)
fx.normal.samples <- (1/3)*dnorm(theta,mean=-2,sd=1) + (2/3)*dnorm(theta,mean=2,sd=1) 
gx.normal.samples <- dnorm(theta,mean=0,sd=3)

plot(theta, fx.normal.samples, ylim=c(0,1.1*max(fx.normal.samples)),
      type="l", xlab="theta", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2)
M <- 1
lines(theta, M*gx.normal.samples)
```

#### Part D

First explore the expectation of $\mu_1$.
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = E\left(\frac{1}{3}\text{Norm}(-2,1)\right)+E\left(\frac{2}{3}\text{Norm}(2,1)\right)$$
$$\mu_1 = \frac{1}{3}(-2)+\frac{2}{3}(2) = \frac{2}{3}$$

Next investigate the expectation of $\mu_2$.
$$\mu_2 = E\left(\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)^2\right)$$

Finally let us compute the expectation of $\theta$.
$$\theta = E\left(\exp\left(\frac{1}{3}\text{Norm}(-2,1)+\frac{2}{3}\text{Norm}(2,1)\right)\right)$$
```{r, imp.sampD}
#store the theoretically calculated values in R
mu1True <- 2/3 
mu2True <- 5 
thetaTrue <- (exp(-3/2)/3) + (2*exp(5/2)/3)
```

#### Part E
```{r, imp.sampE}
## write f and g as r functions
fx <- function(x){
  (1/3)*dnorm(x,mean=-2,sd=1) + (2/3)*dnorm(x,mean=2,sd=1) 
} 
gx <- function(x){
  dnorm(x,mean=0,sd=3)
} 

## write them taking the log
logfx <- function(x){
  log((1/3)*dnorm(x,mean=-2,sd=1) + (2/3)*dnorm(x,mean=2,sd=1)) 
} 
loggx <- function(x){
  log(dnorm(x,mean=0,sd=3))
} 
gSampleFunc <- function(){ rnorm( 1 , mean = 0 , sd = 3 ) }
# Function to calculate the mean of the function h(x):
muhat <- function(hFunc, ISObject){ 
  return(sum(hFunc(ISObject[[1]]) * exp(ISObject[[2]]))/sum(exp(ISObject[[2]])))
  }
####importance sampling without rejection control
mixNormSampler <- ImpSampler(5000, logfx, loggx, gSampleFunc)
mu1 <- function(x){x}
mu2 <- function(x){x^2}
theta <- function(x){exp(x)}
estimates = c(muhat(mu1, mixNormSampler), muhat(mu2, mixNormSampler), muhat(theta, mixNormSampler))
estimates
# 0.7225219 4.9041803 8.0581787
abs(estimates - c(mu1True,mu2True,thetaTrue))   #error
# [1] 0.05585519 0.09581972 0.13786070
mixNormSampler[[3]] # The ESS:
# [1] 3185.469
```

#### Part F
```{r, imp.sampF}
####importance sampling with rejection control (1:10)
#initialize
accRates <- NULL ; effSSize <- NULL ; mu1Errors <- NULL ; mu2Errors <- NULL ; thetaErrors <- NULL
#run through and store output with varying values for the rejection control as directed
for (i in 1:10) {
  mixNormSamp <- ImpSampler(5000, logfx, loggx, gSampleFunc, rejectionControlConstant = i)
  accRates <- c(accRates, mixNormSamp[[4]])
  effSSize <- c(effSSize, mixNormSamp[[3]])
  mu1Errors <- c(mu1Errors, abs(muhat(mu1, mixNormSamp) - mu1True))
  mu2Errors <- c(mu2Errors, abs(muhat(mu2, mixNormSamp) - mu2True))
  thetaErrors <- c(thetaErrors, abs(muhat(theta, mixNormSamp) - thetaTrue))
}
round(rbind(mu1Errors, mu2Errors, thetaErrors, accRates, effSSize), 2)
#             [,1]    [,2]    [,3]    [,4]    [,5]    [,6]    [,7]    [,8]    [,9] [,10]
#mu1Errors    0.02    0.04    0.06    0.04    0.02    0.03    0.02    0.01    0.04 2e-02
#mu2Errors    0.02    0.09    0.14    0.09    0.04    0.06    0.04    0.05    0.03 2e-02
#mu3Errors    0.02    0.31    0.24    0.20    0.17    0.17    0.20    0.10    0.06 5e-02
#accRates     0.70    0.47    0.34    0.25    0.20    0.17    0.14    0.12    0.11 1e-01
#effSSize  4347.45 4956.41 5000.00 5000.00 5000.00 5000.00 5000.00 5000.00 5000.00 5e+03


par(mfrow = c(1, 3))
plot(mu1Errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu1", ylab = "Absolute error")
plot(mu2Errors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu2", ylab = "Absolute error")
plot(thetaErrors, type = "l", xlab = "Rejection control constant", main = "Plot of errors in estimating mu3", ylab = "Absolute error" )


par(mfrow = c(1, 2))
plot(accRates, ylim = c(0, 1), type = "l", xlab = "Rejection control constant", main = "Plot of acceptance rates", ylab = "Acceptance rate")
plot(effSSize, ylim = c(0, 5000), type = "l", xlab = "Rejection control constant", main = "Plot of ess", ylab = "ess")
#accRates decreasing with C
#effSSize increading with C
```



## 2. Chapter 10 Question 5
#### Part A
Indeed, we are given the structure of the model from the problem statement. The information we have is as follows, where $J=10$:
$$p(y_j|\theta_j) \sim \text{Bin}(n_j,\theta_j), \text{ i.e.   } p(y|\theta,\alpha,\beta) = \prod_{j=1}^{10}{n_j \choose y_i}\theta_j^{y_i}(1-\theta_j)^{n_j-y_i}$$
$$p(\theta_j) \sim \text{logit}^{-1}(\alpha+\beta x_j)$$
$$\alpha \sim t_4(0, 2^2) \text{ and } \beta \sim t_4(0, 1^2)$$
$$x_j \sim U(0, 1) \text{ and } n_j \sim \text{Poiss}(5)$$
I build the dataset of 10 samples from this model as directed (Table 1).
```{r, 10.5A}
###10.5A
#build random dataset from the model
yis <- c(rep(NA, 10))
nis <- c(rep(NA, 10))
xis <- c(rep(NA, 10))
alphais <- c(rep(NA, 10))
betais <- c(rep(NA, 10))

set.seed(1)
for (i in 1:10) {
  alpha <- rmt(1,0,2,4)
  alphais[i] <- alpha
  
  beta <- rmt(1,0,1,4)
  betais[i] <- beta
  
  xi <- runif(1, min=0, max=1)
  xis[i] <- xi
  
  ni <- rpois(1,5)
  nis[i] <- ni
  
  p <- inv.logit(alpha + beta*xi)
  
  yis[i] <- rbinom(1, ni, p)
}

dataset <- cbind(yis,nis)
dataset.df <- t(data.frame(dataset))
kable(dataset.df, "latex", caption = "10.5 Sampled Data", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part B
Next, I used rejection sampling to acquire to get 1000 independent posterior draws from $(\alpha, \beta)$. The likelihood follows:
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta)\prod_{i=1}^k p(y_i|\alpha,\beta,n_i,x_i)$$
$$p(\alpha,\beta|y,n,x) \propto p(\alpha,\beta) \prod_{i=1}^k [\text{logit}^{-1}(\alpha+\beta x_i)]^{y_i}[1-\text{logit}^{-1}(\alpha+\beta x_i)]^{n_i-y_i}$$
For simplitcity's sake we will let the prior distribution equal the inverse of the normalization constants (is this what you said, Jerson? I can't quite remember if this was what you recommended for forgoeing the specification of a prior but I can't think of what else you might have said...). Given that we also have no knowledge that would given us reason to choose a particular prior, this is as appropriate a choice as any. Therefore, I calculated the posterior as follows:
$$q(\alpha,\beta|y,n,x) \propto \prod_{i=1}^k [\text{logit}^{-1}(\alpha+\beta x_i)]^{y_i}[1-\text{logit}^{-1}(\alpha+\beta x_i)]^{n_i-y_i}$$


First, I display the contours of the joint posterior distribution of $(\alpha, \beta)$ for your reference. The following plot shows the draws from the rejection sampling.

```{r, 10.5B}
###10.5B
library(LearnBayes)
#posterior function
logitBin = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  N = length(xis)
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*xis)) + 
    (n-y)*log((1-inv.logit(alpha + beta*xis)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  post = loglikelihood #+ logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 30
x0 = seq(-5, 5, len = ng)
y0 = seq(-15, 10, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin, data = dataset)
Z = Z - max(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Contour plot of joint posterior")

####Rejection sampling
#find the mode
tpar=LearnBayes::laplace(logitBin,array(c(3,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar) {
	data=data
	d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,
	log=TRUE)
	return(d)
}

start=array(c(3,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT, start, data=dataset, tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

rejectsamp=function(logf,tpar,dmax,n,data){
	theta=rmt(n,mean=c(tpar$mode),S=2*tpar$var,df=4)
	lf=apply(theta,1,logf,data=data)
	lg=dmt(theta,mean=c(tpar$mode),S=2*tpar$var,df=4,log=TRUE)
	prob=exp(lf-lg-dmax)
	return(theta[runif(n)<prob,])
}

theta=rejectsamp(logitBin,tpar,logitBinT(fit1$mode,dataset,tpar),2000,dataset)
#dim(theta)
contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2,
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Rejection Sampling",
        drawlabels = F)
points(theta[,1],theta[,2])
```

#### Part C

Here we use the Laplace function to estimate the approximate the posterior density for $(\alpha,\beta)$ with a normal centered at the posterior mode with covariance matrix fit to the curvature at the mode. See below for the estimates:

```{r, 10.5C}
###10.5C
# build normal approximation
tpar=LearnBayes::laplace(logitBin,array(c(-1,1),c(1,2)),data=dataset)
logitBinT=function(theta,data,tpar) {
  data=data
  d=logitBin(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
  return(d)
}
start=array(c(-1,1),c(1,2))
fit1=LearnBayes::laplace(logitBinT,start,data=dataset,tpar=tpar)
#logitBinT(fit1$mode,dataset,tpar)

dataset.df10.5C1 <- data.frame(fit1$mode)
colnames(dataset.df10.5C1) <- c("alpha", "beta")
rownames(dataset.df10.5C1) <- c("")
kable(dataset.df10.5C1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

dataset.df10.5C2 <- data.frame(fit1$var)
colnames(dataset.df10.5C2) <- c("alpha", "beta")
rownames(dataset.df10.5C2) <- c("alpha", "beta")
kable(dataset.df10.5C2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part D
Using the calculated posterior mode and covariance matrix to build two-dimensional $t_4$ distributions, I employed importance sampling to estimate $E(\alpha|y)$ and $E(\beta|y)$. First, see the samples displayed on the contour plot. The table below that displayes the distribution of the estimates for $(\alpha,\beta)$. The means of the 1,000 samples were calculated as 0.205 for $\alpha$ and -2.01 for $\beta$.

```{r, 10.5D}
###10.5D
##importance sampling using normal centered at the posterior mode with covariance 
    #matrix fit to the curvature at the mode
theta = rmt(1000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin,data=dataset)
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
wt = exp(lf - lp)
probs=wt/sum(wt, na.rm = T)
probs[is.na(probs)] <- 0
theta.S=theta[sample(1:1000,size=1000,prob=probs,replace=TRUE),]

contour(x0, y0, Z, levels = c(-30, -20, -10, -5, -1, -0.1, -0.01), lwd = 2, 
        xlab="alpha", 
        ylab="beta",
        main="Posterior Draws from Importance Sampling",
        drawlabels = F)
points(theta.S[,1],theta.S[,2])

#estimates
dataset.df10.5D <- t(data.frame(apply(theta.S,2,summary)))
rownames(dataset.df10.5D) <- c("alpha", "beta")
kable(dataset.df10.5D, "latex", caption = "10.5 Estimates for Expectations", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

#### Part E
I calculated the effective sample size to be 1.63. Given it's small size this would lead me to believe that there are likely a few extremely high weights which might be unduly influencing the distribution.

```{r, 10.5E}
###10.5E
normalize = function(x){return(x/sum(x))}
seff <- 1/sum(normalize(wt)^2)
#seff <- length(wt)/(1 + var(wt))
#seff
```


## 3. Chapter 10 Question 8
#### Part A

First, I set out to recreate the approximate posterior distribution of the bioassay example in Section 3.7. The model has the same structure as was written out for Problem 10.7. I will forgoe re-writing it here. My first plot below is my recreation of Figure 3.3b to confirm that I am recreating the distribution correctly. After sampling 10,000 from the approximate distribution, I resampled without replacement $k = 1000$ samples. The plot of those draws are shown on the next plot.

```{r, 108A}
###10.8A
# load data
x108 <- c(-0.86,-0.30,-0.05,0.74) 
n108 <- rep(5,length(x108))
y108 <- c(0,1,3,5)
dataset108<-cbind(y108,n108)

#posterior function
logitBin108 = function (theta, data)
{
  alpha <- theta[1]
  beta <- theta[2]
  y = data[,1]
  n = data[,2]
  prelikelihood <- function (y, n, alpha, beta) y*log(inv.logit(alpha + beta*x108)) + 
    (n108-y)*log((1-inv.logit(alpha + beta*x108)))
  loglikelihood <- sum(prelikelihood(y, n, alpha, beta))
  logPrior <- 0 + 0 #non informative 
  post = loglikelihood + logPrior
  return(post)
}

#look at contours of posterior function across grid of alphas and betas
ng = 50
#grid suggestion from book
x0 = seq(-2, 7, len = ng)
y0 = seq(-2, 35, len = ng)
X = outer(x0, rep(1, ng))
Y = outer(rep(1, ng), y0)
n2 = ng^2
Z = apply(cbind(X[1:n2], Y[1:n2]), 1, logitBin108, data = dataset108)
Z = exp(Z)
Z = matrix(Z, c(ng, ng))
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Contour Plots to Compare to Figure 3.3b",
        drawlabels = F)

# build normal approximation
tpar=LearnBayes::laplace(logitBin108,array(c(1,10),c(1,2)),data=dataset108)
betabinT108=function(theta,data,tpar)
{
	data=data
	d=logitBin108(theta,data)-dmt(theta,mean=c(tpar$mode), S=2*tpar$var,df=4,log=TRUE)
	return(d)
}
start=array(c(1,10),c(1,2))
fit1=LearnBayes::laplace(betabinT108,start,data=dataset108, tpar=tpar)
#betabinT108(fit1$mode,dataset108,tpar)

#importance resampling SIR without replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=F),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling Without Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```

#### Part B
See below for my first attempt at displaying the distribution of the simulated importance ratios. 

```{r}
###10.8B
hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios with Outliers")
```

For a more representative display of the simulated importance ratios, I remove multiple outliers from the upper end of the distribution. The resulting distribution is displayed below. The importance ratios appear to ratios vary as we would hope! 

```{r}
#remove outliers
for(i in 1:40){
  wt<-wt[wt < max(wt)]
}

hist(wt, main="Distribution of the Simulated Importance Ratios",
     xlab="Simulated Importance Ratios without Outliers")
```

#### Part C

Below I display the results from doing importance resampling with replacement. There does not appear to be much difference between the samples drawn when replacing that in Part A where there was none. This is an indicator that the importance weights are moderate, whichi is in line with what we saw in Part B. In situations like these, it would make sense that sampling with and without replacement gives similar results (as compared to a situation where there are a few large weights and many small weights and sampling with replacement would pick the same few values repeatedly, leading to a different set that a more conservative without replacement approach).

```{r, 108C}
###10.8C
#importance resampling SIR WITH replacement
theta = rmt(10000, mean = c(tpar$mode), S = tpar$var, df = 4)
lf = apply(theta,1,logitBin108,data=dataset108)
lf[is.na(lf)] <- 0
lp = dmt(theta, mean = c(tpar$mode), S = tpar$var, df = 4,log = TRUE)
md = max(lf - lp)
wt = exp(lf - lp)
probs=wt/sum(wt)
theta.S=theta[sample(1:length(probs),size=1000,prob=exp(probs),replace=T),]
#apply(theta.S,2,summary)

contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        main="Importance Resampling With Replacement",
        drawlabels = F)
points(theta.S[,1],theta.S[,2], pch='.')
```




## 4. Chapter 11 Question 2
Following up on questions 10.7 and 10.8, this problem encourages us to try sampling using the the Metropolis algorithm. First, I defined my starting points and my jumping rule. I used a normal approximation to construct estimates for these parameters (Table 5).


```{r, 112}
###11.2
#initializations
sims <- 1000
theta <-  matrix(0, nrow = sims, ncol = 2) 
r.all <- rep(0,sims) 

#starting positions found from normal approximation
theta[1,] <- c(0.847,7.749)

dataset.df11.2.1 <- t(data.frame(theta[1,]))
colnames(dataset.df11.2.1) <- c("alpha", "beta")
rownames(dataset.df11.2.1) <- c("")
kable(dataset.df11.2.1, "latex", caption = "Estimated Modes", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#jumping function
jumping <- function(theta, scale=.1) rmnorm(1, mean = c(tpar$mode), varcov = matrix(tpar$var,2,2))

dataset.df11.2.2 <- data.frame(matrix(tpar$var,2,2))
colnames(dataset.df11.2.2) <- c("alpha", "beta")
rownames(dataset.df11.2.2) <- c("alpha", "beta")
kable(dataset.df11.2.2, "latex", caption = "Estimated Covariance Matrix", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

Now, we use the metropolis algorithm to pull a set of samples. These samples are displayed below. They seem to be an improvement over the importance sampling attempts.

```{r, 1121}
#metropolis algorithm
for(i in 2:sims) {
  theta.new = jumping(theta[i-1,], .1)
  uu = runif(1)
  r <- min(exp(logitBin108(theta.new, dataset108) - logitBin108(theta[i-1,], dataset108)), 1)
  if (uu < r) {
    theta[i,] <- theta.new
  }else{
    theta[i,] = theta[i-1,]
  }
}

#samples/walks
contour(x0, y0, Z, levels = c(seq(4.822959e-39, 2.737619e-03, length.out=10)), 
        lwd = 1, ylab = "beta", xlab= "alpha",
        drawlabels = F)
points(theta[,1],theta[,2], pch='.')
```

Lastly, we are instructed to check if we have approximated convergence. Below I display the values of alpha and beta as we progress through the iterations. It doesn't appear to me that they are converging to the means as we might hope. I wonder if I have coded something wrong, or if this is simply the best that this method can do?

```{r, 1122}
# plots for convergence? / investigation / display
firstChain <- c(rep(NA, sims))
for (i in 1:sims) {
  firstChain[i] <- exp(logitBin108(theta[i,],dataset108))
}
# probability
#plot(seq(1:sims),firstChain,type="l",xlab="Iteration",
    #ylab="posterior prob",main="post convergence?")
#alpha
plot(seq(1:sims),theta[,1],type="l",xlab="Iteration",
     ylab="Sampled Alpha",main="Alpha Convergence")
#beta
plot(seq(1:sims),theta[,2],type="l",xlab="Iteration",
     ylab="Sampled Beta",main="Beta Convergence")
```


## 5. Chapter 11 Question 3

The question asks us to compare the results from using three types of models on the given data: the hierarchical normal model, the seperate model, and the pooled model. To construct the hierarchical model I used the given conditional densities in the BDA3 text (equations 11.9-11.7). I will not re-write them here to save time, but they are coded out very clearly in the code appendix. The distributions of the output from the hierarchical model are displayed below. I see that the estimates are slightly off from what we might hope. I have combed through my code and can't seem to find what might be causing the discrepancy. I did take the time to try and code my gibbs sampler myself, it's possible that something is going slightly wrong in there?

```{r, 113}
###11.3
#load in data
dataset113 <-  data.frame(machineno = rep(1:6, each=5), 
                          ments = c(83, 92, 92, 46, 67,
                                    117, 109, 114, 104, 87,
                                    101, 93, 92, 86, 67,
                                    105, 119, 116, 102, 116,
                                    79, 97, 103, 79, 92,
                                    57, 92, 104, 77, 100))
J <- length(unique(dataset113$machineno))
n <- length(dataset113$ments)

#starting points
# from text "obtain 10 starting points for the simulations by drawing thetaj 
    #independently in this way for each group"

set.seed(142857)
theta.start <- sapply(1:6,function(x) sample(dataset113$ments[dataset113$machineno==x], 10, replace=TRUE))
mu.start <- apply(theta.start, 2, mean)
sigma.start <- sqrt(apply(theta.start, 2, var))

# ____________________________________________________________________________________________________________________

#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}
#sigma (step 3)
sigma.post.sample <-  function(theta) {
  #first find sigma hat
  sigma.hat.func <- function(theta) {
    sigma.hat <- sapply(1:6, function(x) (dataset113$ments[dataset113$machineno==x] - theta[x])^2)  
    sigma.hat <- (1/n) * sum(unlist(sigma.hat))
    return(sigma.hat)
  }
  #next find conditional sigma
  sigma.hat <- sigma.hat.func(theta) 
  sigma.cond <- rinvchisq(1,n,sigma.hat)
  return(sigma.cond)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma)+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6", 
                               "mu", "sigma2", "tau2")
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,9] <- tau.post.sample(sample.i)
  param.storage[1,8] <- sigma.post.sample(sample.i)
  param.storage[1,7] <- mu.post.sample(sample.i,param.storage[1,9])#param.storage[1,9])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,7], param.storage[i-1,8], param.storage[i-1,9])
    param.storage[i,9] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,8] <- sigma.post.sample(param.storage[i,1:6])
    param.storage[i,7] <- mu.post.sample(param.storage[i,1:6], param.storage[i,9])
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,8:9] <- sqrt(gibbs.output[,8:9])

#summary table
df.11.3.output <- data.frame(t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975)))))
colnames(df.11.3.output) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(df.11.3.output, "latex", caption = "Hierarchical Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

First, they ask us to report (i) the posterior distribution of the mean of the quality measurements of the sixth machine for each of the three models. I referred to the construction of the hierarchical distribution above. I drew from this model by drawing samples from: $N(\theta_6,|\sigma^2)$. To build the "seperate" model, I simply calculated the mean and variance of the sixth machine measurements without taking into account any of the other machine values and then used them as the parameters of the normal distribution, i.e. $N(\bar{y_{6.}},V^{sep}_6)$. Finally, to build the "pooled" model, I simply calculated the mean across all of the samples and the variance of the sixth machine measurements compared to the overall mean, i.e. $N(\bar{y_{..}},V^{pooled}_6)$. These distributions are displayed below. Notably, they appear as we might expect: the hierarchical has the highest density around the sampled mean of the sixth machine, the pooled distribution is pulled higher by the other machine values, and the seperated has the widest spread.

```{r, pooled/sep calcs, i}
#measures for theta 6 for seperate model
y.bar.6.dot <- mean(dataset113$ments[dataset113$machineno==6]) 
var.sep.6 <- var(dataset113$ments[dataset113$machineno==6])
theta.sep.6 <- rnorm(sims, y.bar.6.dot, sqrt(var.sep.6))

#measures for theta 6 for pooled model
y.bar.dot.dot <- mean(dataset113$ments)
var.pooled <- sum((dataset113$ments[dataset113$machineno==6] - mean(dataset113$ments))^2) / 
  (length(dataset113$ments) - 1)
theta.pooled.6 <- rnorm(sims, y.bar.dot.dot, sqrt(var.pooled))

# posterior distribution of the mean of the quality measurements of the sixth machine
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="(i) Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))
```

Next, they ask us to report (ii) the predictive distribution for another quality measurement of the sixth machine. I was a bit unsure what they were asking for here, but  I decided to simply compare how each of the three models are performing by calculating p-values for a text statistic of the mean of the sixth machine. The observed values is displayed with a black line on the plot below. The p-values are displayed below the graph for your reference. They suggest that the hierarchical model and separate model will predict the mean of the sixth machine best.

```{r, ii}
# predictive distribution for another quality measurement of the sixth machine (maximum?)
plot(density(gibbs.output[,"theta6"]), col="red", 
     xlab="Mean Measure", 
     ylab="Density", 
     main="(ii) Mean of Machine 6")
lines(density(theta.pooled.6[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.6[seq(sims/2+1, sims, 1)]), col="green")
lines(x=c(y.bar.6.dot,y.bar.6.dot),y=c(-1,1), col="black")
legend("topright",col = c("red","blue", "green"),
       legend=c("hierarchical","pooled", "separated"),
       lty = c(1,1,1))

pvalmean1 <- length(which(gibbs.output[,"theta6"] > mean(y.bar.6.dot))) / 
  length(gibbs.output[,"theta6"])

pvalmean2 <- length(which(theta.pooled.6[seq(sims/2+1, sims, 1)] > mean(y.bar.6.dot))) / 
  length(theta.pooled.6[seq(sims/2+1, sims, 1)])

pvalmean3 <- length(which(theta.sep.6[seq(sims/2+1, sims, 1)] > mean(y.bar.6.dot))) / 
  length(theta.sep.6[seq(sims/2+1, sims, 1)])

df.11.3.pvals <- data.frame(pvalmean1, pvalmean2, pvalmean3)
colnames(df.11.3.pvals) <- c("Hierarchical", "Pooled", "Seperate")
rownames(df.11.3.pvals) <- c("P-Value")
kable(df.11.3.pvals, "latex", caption = "Posterior Predictive Check", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```

Lastly, they ask us to report (iii) the posterior distribution of the mean of the quality measurements of the seventh machine. As we are given no information about the seventh machine, such as if it shares any characteristics with one of the sixth machines in our dataset, we take a cautious approach and create distributions using the grand means of the distributions. In practice, this means we pull from the following distributions: Hierarchical: $N(\mu,\tau)$, Pooled: $N(\bar{y}_{..},V_{pooled}$ and Seperate: a multinomial model made out of the normal distributions constructed from the mean and variance of each of the six models. These distributions are displayed below. We can see that the hierachical model gives the strongest suggestions by far. 

```{r, iii}
#measures for machine 7 for seperate model
mach1<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==1]),
             sqrt(var(dataset113$ments[dataset113$machineno==1])))
mach2<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==2]),
             sqrt(var(dataset113$ments[dataset113$machineno==2])))
mach3<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==3]),
             sqrt(var(dataset113$ments[dataset113$machineno==3])))
mach4<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==4]),
             sqrt(var(dataset113$ments[dataset113$machineno==4])))
mach5<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==5]),
             sqrt(var(dataset113$ments[dataset113$machineno==5])))
mach6<-rnorm(sims*(1/6),mean(dataset113$ments[dataset113$machineno==6]),
             sqrt(var(dataset113$ments[dataset113$machineno==6])))
theta.sep.7 <- c(mach1,mach2,mach3,mach4,mach5,mach6)

#measures for machine 7 for pooled model
y.bar.dot.dot <- mean(dataset113$ments)
var.pooled <- var(dataset113$ments)
#sum((dataset113$ments[dataset113$machineno==6] - mean(dataset113$ments))^2) / 
 # (length(dataset113$ments) - 1)
theta.pooled.7 <- rnorm(sims, y.bar.dot.dot, sqrt(var.pooled))

#hierarchical
theta.hier.7 <- rnorm(sims, gibbs.output[,"mu"], sqrt(gibbs.output[,"sigma2"]))

# posterior distribution of the mean of the quality measurements of the potential seventh machine
plot(density(theta.hier.7), col="red", xlab="Mean Measure", 
    ylab="Density", main="(iii) Mean of Potential Machine 7")
lines(density(theta.pooled.7[seq(sims/2+1, sims, 1)]), col="blue")
lines(density(theta.sep.7[seq(99, 198, 1)]), col="green")
legend("topright",col = c("red","blue", "green"), 
       legend=c("hierarchical","pooled", "separated"), 
       lty = c(1,1,1)) 
```


## 6. Chapter 11 Question 4

Derivation of $\sigma_j$ form here. 

Explanation of use of $\sigma_0$ grid here and concerns. Derivation of $\sigma_0$ form?

```{r, 114}
###11.4
#heirarchical model functions
#tau (step 4)
tau.post.sample <- function(theta) {
  #first find tau hat
  tau.hat.func <- function(theta) {
    mu <- mean(theta)
    tau.hat <- (1/(J-1)) * sum((theta-mu)^2)
    return(tau.hat)
  }
  #next find conditional tau
  tau.hat <- tau.hat.func(theta) 
  tau.cond <- rinvchisq(1,J-1,tau.hat)
  return(tau.cond)
}

# change sigma function
sigma.post.sample.p2 <- function(theta, mu) {
  sigma2.star <- NULL
  for(j in 1:J)	{
    k <- 5#length( y )
    nu.k <- 1 + k
    sigma.0.grid <- seq(0.25, 30, 0.25)
    sigma2.star.vals<-NULL
    probs <- NULL
    for (i in 1:length(sigma.0.grid)) {
      v <- mean((theta[j] - mu)^2)
      sigma2.k.option <- (1 * sigma.0.grid[i] + k*v) / (1 + k) #nu.0=1
      sigma2.star.vals[i] <- rinvchisq(1, nu.k, sigma2.k.option)
      probs[i] <- dinvchisq(sigma2.star.vals, nu.k, sigma2.k.option)
    }
    #print(mean(probs))
    sigma2.star[j] <- sample(sigma2.star.vals, 1, prob=probs)#rinvchisq(1, nu.k, sigma2.k)  
  }
  return(sigma2.star)
}

#mu (step 2)
mu.post.sample <- function(theta,tau) {
  #first find mu hat
  mu.hat <- mean(theta)
  #next find conditional mu
  mu.cond <- rnorm(1,mu.hat,sqrt(tau/J))
  return(mu.cond)
}

#theta (step 1)
theta.post.sample <- function(mu,sigma,tau){
  theta <- NULL
  for(j in 1:J)	{
    n.j  <- length(dataset113$ments[dataset113$machineno==j])
    y.bar.j <- mean(dataset113$ments[dataset113$machineno==j])
    #first find V hat
    V.hat <- 1 / ((n.j/sigma[j])+(1/tau))
    #next find theta hat
    theta.hat <- (((y.bar.j*n.j)/sigma[j]) + (mu/tau)) * V.hat
    #last find conditional theta
    theta[j] <- rnorm(1,theta.hat,sqrt(V.hat))
  }
  return(theta)
}

# ____________________________________________________________________________________________________________________

#gibbs sampler function
sims <- 200

gibbs.sampler <- function(sample.i) {
  #initializations
  no.params <- 14
  param.storage <- matrix(NA, sims, no.params)
  colnames(param.storage) <- c("theta1", "theta2", "theta3", "theta4", "theta5", "theta6",
                               "sigma1", "sigma2", "sigma3", "sigma4", "sigma5", "sigma6",
                               "tau2","mu")
  #starting values for theta, tau, sigma and mu
  param.storage[1,1:6] <- sample.i
  param.storage[1,13] <- tau.post.sample(sample.i)
  param.storage[1,14] <- mu.post.sample(sample.i,param.storage[1,13])
  param.storage[1,7:12] <- sigma.post.sample.p2(sample.i,param.storage[1,14])
  
  #interations
  for (i in 2:sims) {
    param.storage[i,1:6] <- theta.post.sample(param.storage[i-1,14], param.storage[i-1,7:12], param.storage[i-1,13])
    param.storage[i,13] <- tau.post.sample(param.storage[i,1:6])
    param.storage[i,14] <- mu.post.sample(param.storage[i,1:6], param.storage[i,13])
    param.storage[i,7:12] <- sigma.post.sample.p2(param.storage[i,1:6], param.storage[i,14])
    
  }
return(param.storage)
}
  
#run sampler!
param.storage <- gibbs.sampler(mu.start)
#snag output (second halves as directed in text)
gibbs.output <- param.storage[seq(sims/2+1, sims, 1),]
gibbs.output[,7:13] <- sqrt(gibbs.output[,7:13])

#summary table
df.11.4.output <- data.frame(t(apply(gibbs.output, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975)))))
colnames(df.11.4.output) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(df.11.4.output, "latex", caption = "Hierarchical Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
```


\newpage

## 7* Chapter 13 Question 5
Estimating the number of unseen species (see Fisher, Corbet, and Williams, 1943, Efron and Thisted, 1976, and Seber, 1992): suppose that during an animal trapping expedition the number of times an animal from species i is caught is $x_i \sim Poisson(\lambda_i)$. For parts (a)–(d) of this problem, assume a $Gamma(\alpha,\beta)$ prior distribution for the $\lambda_i$’s, with a uniform hyperprior distribution on $(\alpha,\beta)$. The only observed data are $y_k$, the number of species observed exactly k times during a trapping expedition, for $k = 1, 2, 3,...$

#### Part A
Write the distribution $p(x_i|\alpha,\beta)$.

#### Part B
Use the distribution of $x_i$ to derive a multinomial distribution for y given that there are a total of N species.

#### Part C
Suppose that we are given y = (118, 74, 44, 24, 29, 22, 20, 14, 20, 15, 12, 14, 6, 12, 6, 9, 9, 6, 10, 10, 11, 5, 3, 3), so that 118 species were observed only once, 74 species were observed twice, and so forth, with a total of 496 species observed and 3266 animals caught. Write down the likelihood for y using the multinomial distribution with 24 cells (ignoring unseen species). Use any method to find the mode of $(\alpha,\beta)$ and an approximate second derivative matrix.

#### Part D
Derive an estimate and approximate 95% posterior interval for the number of additional species that would be observed if 10,000 more animals were caught.

#### Part E
Evaluate the fit of the model to the data using appropriate posterior predictive checks.

#### Part F
Discuss the sensitivity of the inference in (d) to each of the model assumptions.










## 8. Generate / simulate n = 120 observations from a three component mixture 
We are instructed to simulate n = 120 observations from a three component mixture Gaussian model where $(p_1, p_2, p_3) = (0.1, 0.3, 0.6)$ and $(\mu_1, \sigma_1^2) = (0,1)$, $(\mu_2, \sigma_21^2) = (-2,2)$, and $(\mu_3, \sigma_3^2) = (3,16)$. See below for the mixture model's distribution where the three means are displayed with black lines. I use k-means clustering algorithm to build a set of labels to initialize my EM algorithm.

```{r, Prep}
set.seed(100)
#investigate model and pull 120 observations as directed
y <- seq(-10,15,0.1)
#dens <- function (x, theta, stnd){dnorm (x, theta, sqrt(1000*theta*(1-theta)))}
dens.mix <- 0.1*dnorm(y,0,1) + 0.3*dnorm(y,-2,sqrt(2)) + 0.6*dnorm(y,3,sqrt(16))
plot (y, dens.mix, ylim=c(0,1.1*max(dens.mix)),
      type="l", xlab="", ylab="", xaxs="i",
      yaxs="i", yaxt="n", bty="n", cex=2,
      main="Gaussian Mixture Model")
lines(x=c(0,0), y=c(0,0.5))
lines(x=c(-2,-2), y=c(0,0.5))
lines(x=c(3,3), y=c(0,0.5))

vals.mix <- c(rnorm(120*0.1,0,1), rnorm(120*0.3,-2,sqrt(2)), rnorm(0.6*120,3,sqrt(16)))
labels <- c(rep(1, 12), rep(2, 36), rep(3, 72))
vals.mix <- data.frame(cbind(vals.mix, labels))

#use k means to initialize
vals.kmeans <- kmeans(vals.mix$vals.mix, 3) 
vals.kmeans.cluster <- vals.kmeans$cluster
vals.df <- data.frame(x = vals.mix$vals.mix, 
                      cluster = vals.kmeans.cluster)

#vals.df %>%
 # dplyr::mutate(num = row_number()) %>%
  #ggplot(aes(y = num, x = x, color = factor(cluster))) +
  #geom_point() +
  #ylab("Values") +
  #ylab("Data Point Number") +
  #scale_color_discrete(name = "Cluster") +
  #ggtitle("K-means Clustering")

vals.summary.df <- vals.df %>%
  dplyr::group_by(cluster) %>%
  dplyr::summarize(mu = mean(x), variance = var(x), std = sd(x), size = n()) %>%
  dplyr::mutate(p = size / sum(size))
```

#### Part A
First, we implement an EM algorithm to find the MLEs for $p_i, \mu_i,$ and $\sigma^2_i$ for $i\in \{1,2,3\}$. I coded three functions: the e-step, the m-step, and a function to loop/iterate between the two. All of the code can be found in the appendix. The output of this algorithm is displayed below both in the table and the figure. They obviously fit the data well but do not end up at the values that we used to build the mixture model, rather they pretty much stay at the values built by the k-means labels. I will discuss this further in Part C. 

```{r, 8A}
###Problem 8 Part A
#e step
e_step <- function(x, mu.vector, sd.vector, p.vector) {
  comp1.prod <- dnorm(x, mu.vector[1], sd.vector[1]) * p.vector[1]
  comp2.prod <- dnorm(x, mu.vector[2], sd.vector[2]) * p.vector[2]
  comp3.prod <- dnorm(x, mu.vector[3], sd.vector[3]) * p.vector[3]
  sum.of.comps <- comp1.prod + comp2.prod + comp3.prod
  comp1.post <- comp1.prod / sum.of.comps
  comp2.post <- comp2.prod / sum.of.comps
  comp3.post <- comp3.prod / sum.of.comps

  sum.of.comps.log.sum <- sum(log(sum.of.comps))

  list("loglik" = sum.of.comps.log.sum,
       "posterior.df" = cbind(comp1.post, comp2.post, comp3.post))
}

#m step
m_step <- function(x, posterior.df) {
  comp1.n <- sum(posterior.df[,1])
  comp2.n <- sum(posterior.df[,2])
  comp3.n <- sum(posterior.df[,3])
  
  comp1.mu <- 1/comp1.n * sum(posterior.df[,1] * x)
  comp2.mu <- 1/comp2.n * sum(posterior.df[,2] * x)
  comp3.mu <- 1/comp3.n * sum(posterior.df[,3] * x)

  comp1.var <- sum(posterior.df[,1] * (x - comp1.mu)^2) * 1/comp1.n
  comp2.var <- sum(posterior.df[,2] * (x - comp2.mu)^2) * 1/comp2.n
  comp3.var <- sum(posterior.df[,3] * (x - comp3.mu)^2) * 1/comp3.n

  comp1.p <- comp1.n / length(x)
  comp2.p <- comp2.n / length(x)
  comp3.p <- comp3.n / length(x)
  
  list("mu" = c(comp1.mu, comp2.mu, comp3.mu),
       "var" = c(comp1.var, comp2.var, comp3.var),
       "p" = c(comp1.p, comp2.p, comp3.p))
}

#loop function
for (i in 1:2000) {
  if (i == 1) {
    # Initialization
    e.step <- e_step(vals.mix$vals.mix, 
                     vals.summary.df[["mu"]], 
                     vals.summary.df[["std"]],
                     vals.summary.df[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    cur.loglik <- e.step[["loglik"]]
    loglik.vector <- e.step[["loglik"]]
    
  } else {
    # Repeat E and M steps till convergence
    e.step <- e_step(vals.mix$vals.mix, m.step[["mu"]], 
                     sqrt(m.step[["var"]]), 
                     m.step[["p"]])
    m.step <- m_step(vals.mix$vals.mix, 
                     e.step[["posterior.df"]])
    loglik.vector <- c(loglik.vector, 
                       e.step[["loglik"]])
    loglik.diff <- abs((cur.loglik - e.step[["loglik"]]))
    if(loglik.diff < 1e-6) {
      break
    } else {
      cur.loglik <- e.step[["loglik"]]
    }
  }
}

#loglik.vector
m.stepdf <- t(data.frame(m.step))
colnames(m.stepdf) <- c("Gaussian 1", "Gaussian 2", "Gaussian 3")
kable(m.stepdf, "latex", caption = "EM Algorithm MLEs", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))


plot_mix_comps <- function(x, mu, sigma, lam) {
  lam * dnorm(x, mu, sigma)
}

data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[1], sqrt(m.step$var[1]), 
                           lam = m.step$p[1]),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[2], sqrt(m.step$var[2]), 
                            lam = m.step$p[2]),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(m.step$mu[3], sqrt(m.step$var[3]), 
                            lam = m.step$p[3]),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("EM Algorithm Fit")
```

#### Part B
Next, we implement an the MCM Gibbs algorithm to find the posterior distributions for $p_i, \mu_i,$ and $\sigma^2_i$ for $i\in \{1,2,3\}$. All of the code can again be found in the appendix. The output of this algorithm is displayed below both in the table and the figure. The Gibbs algorithm seems to do just as well as the EM algorithm. Interestingly, it gets similarly stuck in the same wrong spot. Notably, I used completely uninformative priors (compared to above where I used the k-means clustering labels). If I had more time I would try out other priors and possibly include a function to add more variability to steps' movements.

ADD IN SIGMA PROGRESSION

```{r, 8B}
###Problem 8 Part B
# mixed gaussian model functions

normalize = function(x){return(x/sum(x))}

sample_z = function(x,pi,mu){
  dmat = outer(mu,x,"-") # k by n matrix, d_kj =(mu_k - x_j)
  p.z.given.x = as.vector(pi) * dnorm(dmat,0,1) 
  p.z.given.x = apply(p.z.given.x,2,normalize) # normalize columns
  z = rep(0, length(x))
  for(i in 1:length(z)){
    z[i] = sample(1:length(pi), size=1,prob=p.z.given.x[,i],replace=TRUE)
  }
  return(z)
}

sample.pi.post = function(z,k) {
  counts = colSums(outer(z,1:k,FUN="=="))
  pi = LearnBayes::rdirichlet(1,counts+1)
  return(pi)
}

sample_mu = function(x, z, k, prior){
  df = data.frame(x=x,z=z)
  mu = rep(0,k)
  for(i in 1:k){
    sample.size = sum(z==i)
    sample.mean = ifelse(sample.size==0,0,mean(x[z==i]))
    post.prec = sample.size+prior$prec
    post.mean = (prior$mean * prior$prec + sample.mean * sample.size)/post.prec
    mu[i] = rnorm(1,post.mean,sqrt(1/post.prec))
  }
  return(mu)
}

#gibbs sampler function
sims <- 1000

gibbs.sampler.8 <- function(x, k, muprior = list(mean=0,prec=0.1)) {
  #initializations
  no.params <- 9
  param.storage <- matrix(NA, nrow=sims, ncol=no.params)
  colnames(param.storage) <- c("mu1", "mu2", "mu3",
                               "sigma1", "sigma2", "sigma3",
                               "p1", "p2", "p3")
  z.storage <- matrix(NA, nrow=sims, ncol=length(vals.mix$vals.mix))
  
  #starting values for theta, tau, sigma and mu (calculated from our sample build earlier)
  param.storage[1,1:3] <- c(0,0,0)#vals.summary.df$mu
  param.storage[1,4:6] <- vals.summary.df$variance
  param.storage[1,7:9] <- c(1/3,1/3,1/3)#vals.summary.df$p
  z.storage[1,] <- rep(c(1,2,3),40)#vals.kmeans.cluster
  
  #interations
  for (i in 2:sims) {
    z.storage[i,] = sample_z(x,param.storage[i-1,7:9],param.storage[i-1,1:3])
    param.storage[i,7:9] = sample.pi.post(z.storage[i,],k)
    param.storage[i,1:3] = sample_mu(x,z.storage[i,],k,muprior)
  }
return(param.storage)
return(z.storage)
}

gibbs.output.8 <- data.frame(gibbs.sampler.8(vals.mix$vals.mix, 3))

plot(gibbs.output.8$mu1,ylim=c(-10,15),type="l",main="Gibbs Algorithm Mu Estimates")
lines(gibbs.output.8$mu2,col=2)
lines(gibbs.output.8$mu3,col=3)

#summary table
gibbsgausmix.df <- data.frame(t(apply(gibbs.output.8, 2, function(x) quantile(x, c(.025,.25,.5,.75,.975),na.rm = T))))
colnames(gibbsgausmix.df) <- c("2.5%", "25%", "50%", "75%", "97.5%")
kable(gibbsgausmix.df, "latex", caption = "Gibbs Algorithm Posterior Distribution", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))


data.frame(x = vals.mix$vals.mix) %>%
  ggplot() +
  geom_histogram(aes(x, ..density..), binwidth = 1, colour = "black", 
                 fill = "white") +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu1[500:1000]), sqrt(mean(gibbs.output.8$sigma1[500:1000])), 
                           lam = mean(gibbs.output.8$p1[500:1000])),
                colour = "red", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu2[500:1000]), sqrt(mean(gibbs.output.8$sigma2[500:1000])), 
                            lam = mean(gibbs.output.8$p2[500:1000])),
                colour = "blue", lwd = 1.5) +
  stat_function(geom = "line", fun = plot_mix_comps,
                args = list(mean(gibbs.output.8$mu3[500:1000]), sqrt(mean(gibbs.output.8$sigma3[500:1000])), 
                            lam = mean(gibbs.output.8$p3[500:1000])),
                colour = "green", lwd = 1.5) +
  ylab("Density") +
  xlab("Values") +
  ggtitle("Gibbs Algorithm Posterior Fit")
```

#### Part C
As I mentioned above, both approaches produce very similar results. Unfortunately, neither appear to be able to find the true grouping of normals. I wonder if something is going wrong in my code, if using the k-means labels was potentially an inappropriate way to start my models, my attempts were no better than the one line of code I used to build the k-means labels, or if this is the point of the exercise. If I had more time, I would be interested in testing this out further. However, I am encouraged by getting both models working and that they both create a set of normals that fit our set of 120 samples. 


## 9. Chapter 15 question 3
Regression with many explanatory variables: Table 15.2 displays data from a designed experiment for a chemical process. In using these data to illustrate various approaches to selection and estimation of regression coefficients, Marquardt and Snee (1975) assume a quadratic regression form; that is, a linear relation between the expectation of the untransformed outcome, y, and the variables x1,x2,x3, their two-way interactions, x1x2, x1x3, x2x3, and their squares, x21, x2, x23.

#### Part A
Fit an ordinary linear regression model (that is, nonhierarchical with a uniform prior distribution on the coefficients), including a constant term and the nine explanatory variables above.

First, I made sure to standardize (i.e. mean centering and scaling) my predictors along with my outcome as Marquardt and Snee (1975) recommend that we be sure to do. This helps us deal with the collinearity in the predictors that is quite intuitive given that the last six are derived directly from the first three. Now, we run a basic ordinary linear regression model. I used both a frequentist method and a bayesian method with the defined priors to compare the results. They were consistent, as I would expect them to be. See the beta coefficient estimates for these models below:  

```{r, 153PartA Prep}
#load data
dataset153 <- data.frame(c(1300, 1300, 1300, 1300, 1300, 1300, 1200, 1200, 1200, 1200, 1200, 1200, 1100, 1100, 1100, 1100),
                         c(7.5, 9.0, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 13.5, 17.0, 23.0, 5.3, 7.5, 11.0, 17.0),
                         c(0.0120, 0.0120, 0.0115, 0.0130, 0.0135, 0.0120, 0.0400, 0.0380, 0.0320, 0.0260, 0.0340, 0.0410, 
                           0.0840, 0.0980, 0.0920, 0.0860),
                         c(49.0, 50.2, 50.5, 48.5, 47.5, 44.5, 28.0, 31.5, 34.5, 35.0, 38.0, 38.5, 15.0, 17.0, 20.5, 19.5))
colnames(dataset153) <- c("x1", "x2", "x3", "y")

# standardizations
#get means for later in case
dataset153<-data.frame(scale(dataset153))
```

```{r, MODEL 1 : frequentist model}
# MODEL 1 : frequentist model
freq.mod <- lm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), data = dataset153)
#summary(freq.mod)
output <- summary(freq.mod)$coef[, 1:2]
out1 <- cbind(output, confint(freq.mod)[c(1:10),])
colnames(out1) <- c("50%", "sd", "2.5%", "97.5%")
out1 <- out1[, c(3, 1, 4, 2)]
out1 <- data.frame(round(out1, 2))
colnames(out1) <- c("2.5%", "50%", "97.5%", "Std Dev")

kable(out1, "latex", caption = "Frequentist Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))
#vif(freq.mod)

```

```{r, MODEL 2 : MCMC regress, eval=F}
# MODEL 2 : use simple MCMC regress (uniform priors?)
library(MCMCpack)
part.a.mod2 <- MCMCregress(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                           data = dataset153, burnin = 1000, mcmc = 10000)
summary(part.a.mod2)
plot(part.a.mod2)
```

```{r, MODEL 3 : use rjags, eval=F}
# MODEL 3 : use rjags
library(rjags)
library(R2jags)

dataset153.dat<-list(y=dataset153$y,
                     x1=dataset153$x1, 
                     x2=dataset153$x2, 
                     x3=dataset153$x3)

cat(
    "model{
    for (i in 1:16) {
      y[i] ~dnorm(mu[i] , tau)
      mu[i] <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i]
    }
    beta0 ~dunif(-1000, 1000)
    beta1  ~dnorm(0, 0.00001)
    beta2 ~dnorm(0, 0.00001)
    beta3 ~dnorm(0, 0.00001)
    tau ~ dinvchi (0.001, 0.001)
    sigma2 <- 1/tau}",
    file="m1.jag"
    )

m1.inits<-list(list("beta0"=1,"beta1"=0,"beta2"=0,"beta3"= 0,
          "tau"=1))

parameters <- c("beta0", "beta1", "beta2", "beta3", "mu", 
           "sigma2")

m1 <- jags(data = dataset153.dat,
        inits = m1.inits,
        parameters.to.save = parameters,
        model.file = "m1.jag",
        n.chains = 1,
        n.iter = 5000,
        n.burnin = 2000,
        n.thin = 1)

m1
plot(m1)        
traceplot(m1)       
plot(as.mcmc(m1))
```

```{r, MODEL 4 : use stan, results='hide',fig.keep='all'}
# MODEL 4 : use stan
library(rstan)
library(rstanarm)
library(bayesplot)

#intercept:constant term,fixed effect
#mean center and scale:standardize

invisible(glm_post1 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family=gaussian, 
                      prior=NULL))
#summary(glm_post1)
out2 <- summary(glm_post1)[1:10,3:6]
out2 <- out2[, c(2, 3, 4, 1)]
out2 <- data.frame(round(out2, 2))
colnames(out2) <- c("10%", "50%", "90%", "Std Dev")
  
kable(out2, "latex", caption = "Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
#stan_trace(glm_post1)

#posterior predictive chekcs
#pp_check(glm_post1)
#ppc_intervals(y = dataset153$y, yrep = posterior_predict(glm_post1), x = dataset153$x1)

#stan_hist(glm_post1, pars=c("x1"), bins=40)
post_samps_speed <- as.data.frame(glm_post1, pars=c("x1"))[,"x1"]
mn_speed <- mean(post_samps_speed) # posterior mean 
ci_speed <- quantile(post_samps_speed, probs=c(0.05, 0.95)) # posterior 90% interval 

#stan_hist(glm_post1, pars=c("x2"), bins=40)
#stan_hist(glm_post1, pars=c("x3"), bins=40)
```

#### Part B
Fit a mixed-effects linear regression model with a uniform prior distribution on the constant term and a shared normal prior distribution on the coefficients of the nine variables above. If you use iterative simulation in your computations, be sure to use multiple sequences and monitor their joint convergence.

```{r, 153PartB, results='hide',fig.keep='all'}
invisible(glm_post_mixed <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family = gaussian(), 
                      prior = normal(),
                      prior_intercept = NULL))
#summary(glm_post_mixed)
out3 <- summary(glm_post_mixed)[1:10,3:6]
out3 <- out3[, c(2, 3, 4, 1)]
out3 <- data.frame(round(out3, 2))
colnames(out3) <- c("10%", "50%", "90%", "Std Dev")

kable(out3, "latex", caption = "Hierarchical Normal Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#tau:chisquarred15.4

# Look at trace plots
# Ideally we want the chains in each trace plot to be stable (centered around one value) and well-mixed 
    #(all chains are overlapping around the same value).
#stan_trace(glm_post_mixed)
```

#### Part C
Discuss the differences between the inferences in (a) and (b). Interpret the differences in terms of the hierarchical variance parameter. Do you agree with Marquardt and Snee that the inferences from (a) are unacceptable?

#### Part D
Repeat (a), but with a t4 prior distribution on the nine variables.

```{r, 153PartD, results='hide',fig.keep='all'}
invisible(glm_t4 <- stan_glm(y ~ x1 + x2 + x3 + x1*x2 + x1*x3 + x2*x3 + I(x1^2) + I(x2^2) + I(x3^2), 
                      data = dataset153, 
                      family = gaussian(), 
                      prior=student_t(df = 4),
                      prior_intercept = NULL))

#stan_trace(glm_t4)
#summary(glm_t4)
out4 <- summary(glm_t4)[1:10,3:6]
out4 <- out4[, c(2, 3, 4, 1)]
out4 <- data.frame(round(out4, 2))
colnames(out4) <- c("10%", "50%", "90%", "Std Dev")

kable(out4, "latex", caption = "Hierarchical T4 Bayesian Model Estimates", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position"))

#summary(glm_t4)
```

#### Part E
Discuss other models for the regression coefficients.







# Code Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```